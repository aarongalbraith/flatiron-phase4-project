{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec0d8bb",
   "metadata": {},
   "source": [
    "# Flatiron Phase 4 Project\n",
    "\n",
    "## Aaron Galbraith\n",
    "\n",
    "### Submitted: \n",
    "\n",
    "## Contents\n",
    "\n",
    "- **[Business Understanding](#Business-Understanding)<br>**\n",
    "- **[Data Understanding](#Data-Understanding)**<br>\n",
    "- **[Data Preparation](#Data-Preparation)**<br>\n",
    "- **[Exploration](#Exploration)**<br>\n",
    "- **[Modeling](#Modeling)**<br>\n",
    "- **[iNTERPRET](#iNTERPRET)**<br>\n",
    "- **[Conclusions/Recommendations](#CONCLUSIONS-&-RECOMMENDATIONS)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee5162",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "Apple launched the iPad 2 on March 11, 2011, the same day that the 2011 SXSW Festival began in Austin, TX. Apple also launched a pop-up store in Austin specifically to sell these and other products to the swell of crowds who attended the festival that year.\\* Apple product launches for their lines of iPods, iPhones, and iPads were a very big deal at the time, and much media coverage was devoted to the frenzy that accompanied each launch, e.g. Apple customers eagerly waiting in long lines for the product on the first day it was available for sale.\n",
    "\n",
    "Apple can simply look to its accounting to see how successful its sales were in Austin during SXSW. But there is more to be learned than just how many dollars it made in the short term. By heavily promoting its product launches in an environment such as this festival, Apple encourages its customers (and loyal fans, and potential customers, and even detractors) to join in a conversation about them. This creates a great opportunity for Apple to get candid feedback on a massive scale about what it's doing that excites people as well as what disappoints people.\n",
    "\n",
    "Following the festival, Apple wished to gain insight into how its presence at the festival had been received. Tweets with the hashtag #sxsw were collected and labeled according to 1) what sentiment if any they expressed and 2) which if any tech brands or products (limited to Apple and Google) were mentioned. Apple wanted to know what it could learn not only from its own festival presence but also from Google's presence at the same festival.\n",
    "\n",
    "<sub><sup>\\*Essentially none of this information accompanied the dataset. Every single tweet contained the hashtag #sxsw, and a frequency analysis of the tweets indicated they took place in 2011. Further research yielded websites such as https://techcrunch.com/2011/03/10/ipad-2-sxsw/ and https://googleblog.blogspot.com/2011/03/google-at-sxsw-2011-austin-here-we-come.html, which provided helpful details about the activities of Apple and Google at the 2011 SXSW Festival.</sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4427b1",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "## Import files\n",
    "\n",
    "Here we'll import all the tools we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2283c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from operator import itemgetter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from wordcloud import WordCloud\n",
    "from nltk import TweetTokenizer\n",
    "import string\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0997b6",
   "metadata": {},
   "source": [
    "## Load and briefly explore data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23dc934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# read csv into dataframe\n",
    "df = pd.read_csv('../data/tweets.csv', encoding='latin-1')\n",
    "# show overview of data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796c7b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9093, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show row and column counts\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19ce85f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text                                            9065\n",
       "emotion_in_tweet_is_directed_at                          9\n",
       "is_there_an_emotion_directed_at_a_brand_or_product       4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show how many unique values for each feature\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53bde97",
   "metadata": {},
   "source": [
    "From the above we see that there are evidently some duplicated tweets; there are 4 different \"emotion\" labels; and there are 9 different product or brand labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ab15184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion_in_tweet_is_directed_at\n",
       "iPad                               946\n",
       "Apple                              661\n",
       "iPad or iPhone App                 470\n",
       "Google                             430\n",
       "iPhone                             297\n",
       "Other Google product or service    293\n",
       "Android App                         81\n",
       "Android                             78\n",
       "Other Apple product or service      35\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show value counts for one feature\n",
    "df.emotion_in_tweet_is_directed_at.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4c20d",
   "metadata": {},
   "source": [
    "Apple products seem to be mentioned more than Google products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81a708b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_there_an_emotion_directed_at_a_brand_or_product\n",
       "No emotion toward brand or product    0.59\n",
       "Positive emotion                      0.33\n",
       "Negative emotion                      0.06\n",
       "I can't tell                          0.02\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show normalized value counts for one feature\n",
    "round(df.is_there_an_emotion_directed_at_a_brand_or_product.value_counts(normalize=True),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280362f",
   "metadata": {},
   "source": [
    "There are very few negative emotions expressed. The majority are neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0641264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_there_an_emotion_directed_at_a_brand_or_product\n",
       "No emotion toward brand or product    0.91\n",
       "Positive emotion                      0.05\n",
       "I can't tell                          0.03\n",
       "Negative emotion                      0.01\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show breakdown of sentiment labels for tweets that have no product or brand identified\n",
    "round(df[df.emotion_in_tweet_is_directed_at.isna()] \\\n",
    ".is_there_an_emotion_directed_at_a_brand_or_product.value_counts(normalize=True),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c247d39",
   "metadata": {},
   "source": [
    "For tweets not associated with a brand, most are labeled neutral, but a few are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9b81176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46      Hand-Held Û÷HoboÛª: Drafthouse launches Û÷H...\n",
       "64      Again? RT @mention Line at the Apple store is ...\n",
       "68      Boooo! RT @mention Flipboard is developing an ...\n",
       "90      Thanks to @mention for publishing the news of ...\n",
       "102     ÛÏ@mention &quot;Apple has opened a pop-up st...\n",
       "                              ...                        \n",
       "9043    Hey is anyone doing #sxsw signing up for the g...\n",
       "9049    @mention you can buy my used iPad and I'll pic...\n",
       "9052    @mention You could buy a new iPad 2 tmrw at th...\n",
       "9054    Guys, if you ever plan on attending #SXSW, you...\n",
       "9058    &quot;Do you know what Apple is really good at...\n",
       "Name: tweet_text, Length: 504, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show examples of tweets that DO express emotion but are NOT directed at a specific product\n",
    "df[(df.is_there_an_emotion_directed_at_a_brand_or_product != 'No emotion toward brand or product') &\n",
    "   (df.emotion_in_tweet_is_directed_at.isna())\n",
    "  ].tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e4a24",
   "metadata": {},
   "source": [
    "Clearly some/most of the tweets we can see here *are* associated with brands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca8499c",
   "metadata": {},
   "source": [
    "## Summary of data\n",
    "\n",
    "There are 9,093 records and three features. As there are only 9,065 unique tweets, it appears that there are some duplicates.\n",
    "\n",
    "A little more than one third (3,291) of the tweets are identified as being directed at a particular product or brand associated with either Google or Apple, while the majority do not identify a product or brand.\n",
    "\n",
    "Relatively few records have been identified as having a negative or \"I can't tell\" emotion.\n",
    "\n",
    "For the 5,802 records that don't identify a product or brand, about 9% of them were identified as having something other than \"no emotion\". Upon investigation of these, it appears that some of them mention \"Apple\" or \"iPad\" after all, so evidently some tweets have not been successfully associated with a product or brand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db39a554",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "## Renaming features\n",
    "\n",
    "The column names are a bit cumbersome, so we'll give them new names that are easier to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22feed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df.rename(columns={'tweet_text': 'text',\n",
    "                   'emotion_in_tweet_is_directed_at': 'brand',\n",
    "                   'is_there_an_emotion_directed_at_a_brand_or_product': 'sentiment'},\n",
    "          inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1df711",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0a1aea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>brand</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  text brand                           sentiment\n",
       "6  NaN   NaN  No emotion toward brand or product"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show records with missing text\n",
    "df[df.text.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b61eae",
   "metadata": {},
   "source": [
    "We can't do anything with a record whose text is missing, so we'll drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0abc6075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop records with missing text values\n",
    "df.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a10c0",
   "metadata": {},
   "source": [
    "## Edit values\n",
    "\n",
    "As these tasks may increase the number of duplicate records, we should perform them before we look for those duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5df95e3",
   "metadata": {},
   "source": [
    "### Lower case\n",
    "\n",
    "It's not likely that we'll lose anything important by shifting all the text to lower case, especially given the nature of tweeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a580f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift all text to lower case\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7499c",
   "metadata": {},
   "source": [
    "### Rename sentiments\n",
    "\n",
    "The sentiment labels could be more succinct. We'll change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d628ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_rename = {\n",
    "    \"No emotion toward brand or product\": \"neutral\",\n",
    "    \"Positive emotion\": \"positive\",\n",
    "    \"Negative emotion\": \"negative\",\n",
    "    \"I can't tell\": \"unknown\"\n",
    "}\n",
    "\n",
    "df.sentiment = df.sentiment.apply(lambda x: sentiment_rename[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef38ac",
   "metadata": {},
   "source": [
    "### Merge brand labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c3efeee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brand\n",
       "iPad                               946\n",
       "Apple                              661\n",
       "iPad or iPhone App                 470\n",
       "Google                             430\n",
       "iPhone                             297\n",
       "Other Google product or service    293\n",
       "Android App                         81\n",
       "Android                             78\n",
       "Other Apple product or service      35\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show breakdown of brand before merging\n",
    "df.brand.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17993048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign either apple or google label and fill in missing values with other\n",
    "df['brand'].replace(['iPad', 'Apple', 'iPad or iPhone App', 'iPhone', 'Other Apple product or service'], 'apple',\n",
    "                     inplace=True)\n",
    "df['brand'].replace(['Google', 'Other Google product or service', 'Android App', 'Android'], 'google',\n",
    "                     inplace=True)\n",
    "df['brand'].fillna('other',\n",
    "                    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8c0c82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brand\n",
       "other     5801\n",
       "apple     2409\n",
       "google     882\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show breakdown of brand after merging\n",
    "df.brand.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962690f5",
   "metadata": {},
   "source": [
    "## Detect missing brand labels\n",
    "\n",
    "As noted earlier, we suspect many of the tweets labeled \"other\" actually refer to a certain product or brand. We'll use some helpful keywords to reclassify some of the tweets that are not yet associated with either brand.\n",
    "\n",
    "In the event that some tweets happen to mention both brands, we'll make a label for \"both\", and we'll label everything else \"neither\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78a6cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make key word lists for apple and google\n",
    "apple_words = ['ipad', 'apple', 'iphone', 'itunes', 'ipad2']\n",
    "google_words = ['google', 'android', 'blogger', 'marissa', 'mayer', 'sketchup', 'h4ckers', 'youtube']\n",
    "# add hashtags\n",
    "apple_hashtags = []\n",
    "google_hashtags = []\n",
    "for word in apple_words:\n",
    "    apple_hashtags.append('#'+word)\n",
    "for word in google_words:\n",
    "    google_hashtags.append('#'+word)\n",
    "apple_words.extend(apple_hashtags)\n",
    "google_words.extend(google_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e09dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function that relabels brand values by finding what keywords are mentioned in the text\n",
    "def brand_fix(text, brand):\n",
    "    # only relabel records that do not have one of the two brands already associated\n",
    "    if brand != 'other':\n",
    "        return brand\n",
    "    else:\n",
    "        apple, google = False, False\n",
    "        # look for apple keyword\n",
    "        for word in apple_words:\n",
    "            if word in text:\n",
    "                apple = True\n",
    "                break\n",
    "        # look for google keyword\n",
    "        for word in google_words:\n",
    "            if word in text:\n",
    "                google = True\n",
    "                break\n",
    "\n",
    "        # return correct new label\n",
    "        if apple & ~google:\n",
    "            return 'apple'\n",
    "        elif google & ~apple:\n",
    "            return 'google'\n",
    "        elif apple & google:\n",
    "            return 'both'\n",
    "        else:\n",
    "            return 'neither'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17d5433f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brand\n",
       "apple      5395\n",
       "google     2831\n",
       "neither     677\n",
       "both        189\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run above function to relabel brand values\n",
    "df['brand'] = df.apply(lambda x: brand_fix(x.text, x.brand), axis=1)\n",
    "# show breakdown of brand after running function\n",
    "df.brand.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca84098",
   "metadata": {},
   "source": [
    "We were able to label a vast majority of the unassociated tweets with a brand that the tweet mentions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5a5e7a",
   "metadata": {},
   "source": [
    "## Duplicates\n",
    "\n",
    "Now we'll address duplicated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abc6ccdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    9050\n",
       "True       42\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show how many records are duplicates\n",
    "df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1b2bf",
   "metadata": {},
   "source": [
    "Let's see if there is a difference if we only select for duplicated text (not product or sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "316e5870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    9047\n",
       "True       45\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show how many records are duplicates for the text value only\n",
    "df.duplicated(subset=['text']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d69ce8a",
   "metadata": {},
   "source": [
    "It looks like 3 text records are duplicated with either different sentiments or different associated brands. Let's look at what sentiment labels these were given, as separate groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8000f0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate group 1 \n",
      " win free ipad 2 from webdoc.com #sxsw rt \n",
      "\n",
      " sentiment\n",
      "neutral     4\n",
      "positive    2\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "- - - -\n",
      "\n",
      "duplicate group 2 \n",
      " rt @mention marissa mayer: google will connect the digital &amp; physical worlds through mobile - {link} #sxsw \n",
      "\n",
      " sentiment\n",
      "neutral     5\n",
      "positive    4\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "- - - -\n",
      "\n",
      "duplicate group 3 \n",
      " rt @mention rt @mention it's not a rumor: apple is opening up a temporary store in downtown austin for #sxsw and the ipad 2 launch {link} \n",
      "\n",
      " sentiment\n",
      "neutral     2\n",
      "positive    1\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "- - - -\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7c19d15d088f>:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  for i, index in enumerate(df.drop_duplicates()[df.duplicated(subset=['text'])].index):\n"
     ]
    }
   ],
   "source": [
    "# show sentiment identification for groups of duplicated tweets\n",
    "for i, index in enumerate(df.drop_duplicates()[df.duplicated(subset=['text'])].index):\n",
    "    print(\n",
    "        'duplicate group', i+1, '\\n',\n",
    "        df.loc[index].text, '\\n\\n',\n",
    "        df[df.text == df.loc[index].text].sentiment.value_counts(),\n",
    "        '\\n\\n- - - -\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7247e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467        before it even begins, apple wins #sxsw {link}\n",
       "468        before it even begins, apple wins #sxsw {link}\n",
       "664     if you're in a room full of people w/good wi-f...\n",
       "775     google to launch major new social network call...\n",
       "776     google to launch major new social network call...\n",
       "798     google to launch major new social network call...\n",
       "2231    marissa mayer: google will connect the digital...\n",
       "2232    marissa mayer: google will connect the digital...\n",
       "2559    counting down the days to #sxsw plus strong ca...\n",
       "3810             win free ipad 2 from webdoc.com #sxsw rt\n",
       "3811             win free ipad 2 from webdoc.com #sxsw rt\n",
       "3812             win free ipad 2 from webdoc.com #sxsw rt\n",
       "3814             win free ipad 2 from webdoc.com #sxsw rt\n",
       "3950    really enjoying the changes in gowalla 3.0 for...\n",
       "3962    #sxsw is just starting, #ctia is around the co...\n",
       "4897    oh. my. god. the #sxsw app for ipad is pure, u...\n",
       "4954               40% of google maps use is mobile #sxsw\n",
       "5338    rt @mention ÷¼ go beyond borders! ÷_ {link} ...\n",
       "5341    rt @mention ÷¼ happy woman's day! make love, ...\n",
       "5842    rt @mention google launching secret new social...\n",
       "5880    rt @mention google to launch major new social ...\n",
       "5881    rt @mention google to launch major new social ...\n",
       "5882    rt @mention google to launch major new social ...\n",
       "5883    rt @mention google to launch major new social ...\n",
       "5884    rt @mention google to launch major new social ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show some of the duplicated tweets\n",
    "df[df.duplicated()].text.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30419063",
   "metadata": {},
   "source": [
    "It's a tough call what to do with these duplicates. Some of them, like the first one, could be multiple people sharing the same article, and it could be meaningful to count all such instances, as they represent *more* of that sentiment. Some others, however, like the \"win free ipad 2\", appear to be from a business promoting itself. In that case, we wouldn't want to skew our results by counting all such instances.\n",
    "\n",
    "In any event, due to the nature of tweeting, it is certainly plausible that these duplicated tweets were not erroneously duplicated, but rather they were actually separate, if identical, tweets when they were posted.\n",
    "\n",
    "Several duplicates we see here start with \"rt\". We know that \"rt\" means \"retweet\", which specifically is a way for Twitter users to amplify a tweet they agree with.\n",
    "\n",
    "Let's compromise on the duplicates by keeping all retweets but dropping the other duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "676097bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine ALL retweets with NON-DUPLICATED other tweets\n",
    "df = pd.concat([\n",
    "    # retweets\n",
    "    df[df.text.str.startswith('rt')],\n",
    "    # non-retweets, minus duplicates\n",
    "    df[~df.text.str.startswith('rt')].drop_duplicates()\n",
    "# reset the index\n",
    "]).reset_index()\n",
    "# drop newly created index column\n",
    "df.drop(columns='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d31024e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9071, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show row and column counts\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7d85b",
   "metadata": {},
   "source": [
    "## Begin NLP\n",
    "\n",
    "Now that we have the data set we want to work with, we'll use natural language processing techniques to help us analyze it.\n",
    "\n",
    "First we'll create a list of all the tweets. As we tokenize and lemmatize, etc, we can always come back to this for the full context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a429d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of all tweet texts\n",
    "corpus = df.text.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee8d93f",
   "metadata": {},
   "source": [
    "Then we'll create a list of all the tokens. To do this, we'll use a tokenizer that is specifically designed to parse tweets from Twitter and a lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77aae7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tokenizer\n",
    "tokenizer = TweetTokenizer(\n",
    "    preserve_case=False,\n",
    "    strip_handles=True\n",
    ")\n",
    "\n",
    "# create list of words from corpus\n",
    "tokens = tokenizer.tokenize(','.join(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a63763",
   "metadata": {},
   "source": [
    "We'll take this opportunity while lemmatizing to get rid of hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4004ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize the list of words\n",
    "tokens_lemmatized = [lemmatizer.lemmatize(word[1:]) for word in tokens if word.startswith('#')] + \\\n",
    "[lemmatizer.lemmatize(word) for word in tokens if not word.startswith('#')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a659cb",
   "metadata": {},
   "source": [
    "Let's look at the most frequently occurring tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44060d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 12561),\n",
       " ('sxsw', 9573),\n",
       " ('.', 5890),\n",
       " ('the', 4423),\n",
       " ('link', 4314),\n",
       " ('}', 4288),\n",
       " ('{', 4285),\n",
       " ('to', 3580),\n",
       " ('at', 3097),\n",
       " ('rt', 2952),\n",
       " ('ipad', 2670),\n",
       " ('a', 2550),\n",
       " ('for', 2544),\n",
       " ('google', 2451),\n",
       " ('!', 2362),\n",
       " ('apple', 2223),\n",
       " ('in', 1936),\n",
       " (':', 1830),\n",
       " ('of', 1711),\n",
       " ('is', 1705),\n",
       " ('\"', 1696),\n",
       " ('and', 1635),\n",
       " ('?', 1611),\n",
       " ('iphone', 1573),\n",
       " ('store', 1518)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the most frequently occurring tokens\n",
    "FreqDist(tokens_lemmatized).most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99044f27",
   "metadata": {},
   "source": [
    "This list is utterly dominated by stopwords. In addition to punctuation characters, some twitter-specific terms appear here, as well as some ordinary stopwords, and of course sxsw. Let's start a stopwords list and put it to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d25e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the standard list of stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "# start our own list of stopwords with these words\n",
    "stop_list = stopwords.words('english')\n",
    "# add to this list some twitter-specific terms\n",
    "stop_list.extend(['sxsw', 'link', 'rt'])\n",
    "# add punctuation characters\n",
    "for char in string.punctuation:\n",
    "    stop_list.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aeec5b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make stopped list of tokens\n",
    "tokens_stopped = [word for word in tokens_lemmatized if word not in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "689f620a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ipad', 2670),\n",
       " ('google', 2451),\n",
       " ('apple', 2223),\n",
       " ('iphone', 1573),\n",
       " ('store', 1518),\n",
       " ('2', 1370),\n",
       " ('new', 1087),\n",
       " ('austin', 956),\n",
       " ('app', 819),\n",
       " ('\\x89', 690),\n",
       " ('launch', 688),\n",
       " ('circle', 685),\n",
       " ('social', 644),\n",
       " ('...', 639),\n",
       " ('android', 588),\n",
       " ('today', 571),\n",
       " ('network', 471),\n",
       " ('get', 453),\n",
       " ('line', 442),\n",
       " ('via', 435),\n",
       " ('pop-up', 422),\n",
       " ('party', 401),\n",
       " ('free', 383),\n",
       " ('called', 358),\n",
       " ('mobile', 343)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the most frequently occurring tokens\n",
    "FreqDist(tokens_stopped).most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1facc289",
   "metadata": {},
   "source": [
    "This looks much better. It seems we've got a few nonsense terms. Let's try to find the most frequently occurring tokens that are not alphanumeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2371fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of tokens that contain non alphanumeric characters\n",
    "tokens_nonalpha = [word for word in tokens_stopped if word.isalpha() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8f63807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2', 1370),\n",
       " ('\\x89', 690),\n",
       " ('...', 639),\n",
       " ('pop-up', 422),\n",
       " ('ipad2', 294),\n",
       " (\"i'm\", 252),\n",
       " (\"google's\", 182),\n",
       " ('\\x9d', 152),\n",
       " ('4', 134),\n",
       " (\"we're\", 127),\n",
       " (':)', 120),\n",
       " ('1', 114),\n",
       " ('6th', 99),\n",
       " (\"apple's\", 95),\n",
       " ('5', 95),\n",
       " (\"i've\", 88),\n",
       " ('÷', 88),\n",
       " ('..', 83),\n",
       " ('3', 80),\n",
       " (\"can't\", 78),\n",
       " ('2011', 75),\n",
       " (\"i'll\", 68),\n",
       " ('11', 67),\n",
       " ('10', 66),\n",
       " (\"there's\", 60)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the most frequently occurring tokens\n",
    "FreqDist(tokens_nonalpha).most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2bac43",
   "metadata": {},
   "source": [
    "It seems quite probable that the \"2\" here is often occurring when tweets include a space in the expression \"ipad 2\". We should trust that this will come up later when we look at collocations / bigrams.\n",
    "\n",
    "Let's look at some context for some of these frequently occurring nonsense terms; it's possible that they strongly correlate with some other particular message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31e6b67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt ' it's 4 p.m. and the #ipad2 line at the apple store is longer and wider ûò about 250 people! only one more hour. ' #sxsw\n",
      "rt ûï@mention going to / at #sxsw? check-in w/friends while youûªre in #austin w/these location-based apps! {link} #lbs\n",
      "rt ûï@mention google to launch major new social network called circles, possibly today {link} #sxswû\n",
      "rt ûï@mention hoot!! *new blog post:* #hootsuite mobile for #sxsw ~ updates for iphone, #blackberry &amp; #android - {link}\n",
      "rt ûï@mention the only news at sxswi was apple's temporary ipad store? #sxsw #wth?û yes. think of it as the ultimate offsite trade show &quot;booth.&quot;\n",
      "rt &gt; @mention msc_page: guy gets tattoo at sxsw so he could win a free ipad2 {link} #sxsw #tattoo #ipadû_ {link}\n",
      "rt@mention by parabolico_bh maravilha, aproveitem!\n",
      "{link} ã_ #edchat #musedchat #sxsw #sxswi #classical\n",
      "rt @mention : aron pilhofer from the new york times just endorsed html over ipad at the #newsapps #sxsw anû_ (cont) {link}\n",
      "rt @mention ûï@mention {link} &lt;-- help me forward this doc to all anonymous accounts, techies,&amp; ppl who can help us jam #libya #sxsw\n",
      "rt @mention ûï@mention &quot;google before you tweet&quot; is the new &quot;think before you speak.&quot; - mark belinsky, #911tweets panel at #sxsw.û\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for tweet in corpus:\n",
    "    if i < 10 and ('\\x89' in tweet or '\\x9d' in tweet):\n",
    "        i += 1\n",
    "        print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84846924",
   "metadata": {},
   "source": [
    "These terms are just confusing and don't indicate anything we can learn from. We'll add a few of the nonsensical terms to our stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b612f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit stop_list\n",
    "stop_list.extend(['\\x89', '...','\\x9d', '÷', '..'])\n",
    "# make stopped list of tokens\n",
    "tokens_stopped = [word for word in tokens_lemmatized if word not in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b9aa69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ipad', 2670),\n",
       " ('google', 2451),\n",
       " ('apple', 2223),\n",
       " ('iphone', 1573),\n",
       " ('store', 1518),\n",
       " ('2', 1370),\n",
       " ('new', 1087),\n",
       " ('austin', 956),\n",
       " ('app', 819),\n",
       " ('launch', 688),\n",
       " ('circle', 685),\n",
       " ('social', 644),\n",
       " ('android', 588),\n",
       " ('today', 571),\n",
       " ('network', 471),\n",
       " ('get', 453),\n",
       " ('line', 442),\n",
       " ('via', 435),\n",
       " ('pop-up', 422),\n",
       " ('party', 401),\n",
       " ('free', 383),\n",
       " ('called', 358),\n",
       " ('mobile', 343),\n",
       " ('sxswi', 342),\n",
       " ('ha', 308)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the most frequently occurring tokens\n",
    "FreqDist(tokens_stopped).most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e909055",
   "metadata": {},
   "source": [
    "By the way, we can use term frequency to try to figure out what year all of this took place. We'll start around the year the first iPhone was released (2007) and include the year the dataset was created (2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7eb7e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007: 0\n",
      "2008: 0\n",
      "2009: 1\n",
      "2010: 4\n",
      "2011: 75\n",
      "2012: 0\n",
      "2013: 0\n"
     ]
    }
   ],
   "source": [
    "# iterate over a likely timespan\n",
    "for year in range(2007,2014):\n",
    "    # show how frequently a given year was mentioned\n",
    "    print(str(year)+':', FreqDist(tokens_stopped)[str(year)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343a96b",
   "metadata": {},
   "source": [
    "This is how we deduced that the data comes from 2011 (which allowed us to learn more context for the SXSW conference in Austin, TX from that particular year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b850f586",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ded28046d863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af3c2c",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "\n",
    "We'll use a number of methods to see what the data can tell us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8651c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_brand_tokens = [word for word in tokens if word not in apple_words and word not in google_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae63e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_apple_pos = list(df[\n",
    "    (df.brand == 'apple') &\n",
    "    (df.sentiment == 'negative')\n",
    "].tokens_lem.explode().astype(str))\n",
    "\n",
    "non_brand_tokens = [word for word in tokens_apple_pos if word not in apple_words and word not in google_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate wordcloud\n",
    "wordcloud = WordCloud(\n",
    "    width=500,\n",
    "    height=300,\n",
    "    collocations = True\n",
    ")\n",
    "\n",
    "# generate wordcloud\n",
    "wordcloud.generate(','.join(non_brand_tokens))\n",
    "\n",
    "# plot wordcloud\n",
    "\n",
    "plt.figure(figsize = (12, 15), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff1682",
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c464d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature of tokenized text\n",
    "df.loc[:, 'text_tokenized'] = df['text'].apply(tokenizer.tokenize)\n",
    "# make vocabulary from set of words\n",
    "vocab_raw = set(df['text_tokenized'].explode())\n",
    "# show size of raw vocabulary\n",
    "print('Size of raw vocabulary:', len(vocab_raw))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da3177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8895fb9",
   "metadata": {},
   "source": [
    "# CONSIDER LABELING SENTIMENT AS A BINARY\n",
    "\n",
    "In order to reduce the class imbalance and generally simplify the work, we'll change the sentiment labels to a binary system of 1 for positive and 0 for everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sentiment binary\n",
    "df['sentiment'].replace({'No emotion toward brand or product': 0,\n",
    "                         'Positive emotion': 1,\n",
    "                         'Negative emotion': 0,\n",
    "                         \"I can't tell\": 0\n",
    "                        }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81458747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show breakdown of sentiment after merging\n",
    "round(df.sentiment.value_counts(normalize=True),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6aae21",
   "metadata": {},
   "source": [
    "## Train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c5c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into target (sentiment) and predictor (text)\n",
    "X, y = df['text'].to_frame(), df['sentiment']\n",
    "# split the data into train and test sets\n",
    "# set random state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012b104",
   "metadata": {},
   "source": [
    "# REVISIT THIS\n",
    "\n",
    "add label to X_train for research purposes .. obviously don't include this in the model\n",
    "\n",
    "reset index to anticipate future problems ... or not reset the index???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3ff843",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:, 'sentiment'] = [y_train.loc[val] for val in X_train.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82006518",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "We'll establish a token pattern that will separate each tweet into a list of tokens at least 2 letters long and ignore punctuation characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414bde9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tokenizer\n",
    "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "tokenizer = RegexpTokenizer(basic_token_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab87e44",
   "metadata": {},
   "source": [
    "We'll make a new column whose values will be the list of tokens from each text value. Then we can group these tokens/words together into a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba95b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature of tokenized text\n",
    "X_train.loc[:, 'text_tokenized'] = X_train['text'].apply(tokenizer.tokenize)\n",
    "# make vocabulary from set of words\n",
    "vocab_raw = set(X_train['text_tokenized'].explode())\n",
    "# show size of raw vocabulary\n",
    "print('Size of raw vocabulary:', len(vocab_raw))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238f009",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "## Baseline model\n",
    "\n",
    "We'll use multinomial naive Bayes for our baseline model and feed it the 10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0083d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "baseline_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb30e34",
   "metadata": {},
   "source": [
    "It will be instructive to recall the percentage of the plurality in the target feature (sentiment). Models should be evaluated in relation to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this value to compare to future model crossval scores\n",
    "plurality_cv = round(y_train.value_counts(normalize=True)[0],4)\n",
    "# show the sentiment breakdown\n",
    "round(y_train.value_counts(normalize=True),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9480e3",
   "metadata": {},
   "source": [
    "Accuracy of future models should exceed the higher of these values, since we know that, at worst, a model could just predict all sentiments are \"nonpositive\" and achieve this accuracy score.\n",
    "\n",
    "We'll run the first model and compare the score to this plurality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c47e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features = 10\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "baseline_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Plurality:', plurality_cv,\n",
    "      '\\nBaseline: ',baseline_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1afbe",
   "metadata": {},
   "source": [
    "This did not improve on plurality at all.\n",
    "\n",
    "Let's look at what words it was using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words(column, title):\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    fig.set_tight_layout(True)\n",
    "    gs = fig.add_gridspec(1, 2)\n",
    "    ax1 = fig.add_subplot(gs[0, :1])\n",
    "    ax2 = fig.add_subplot(gs[0, 1:2])\n",
    "\n",
    "    axes = [ax1, ax2]\n",
    "\n",
    "    for index, category in enumerate(y_train.unique()):\n",
    "\n",
    "        all_words = X_train[X_train['sentiment'] == category][column].explode()\n",
    "        freq_dist = FreqDist(all_words)\n",
    "        top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "        tokens = top_10[0]\n",
    "        counts = top_10[1]\n",
    "\n",
    "        ax = axes[index]\n",
    "        ax.bar(tokens, counts)\n",
    "\n",
    "        ax.set_title(f\"{title} {category}\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax.tick_params(axis=\"x\", rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d81e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_words('text_tokenized', 'sentiment =')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93ecb85",
   "metadata": {},
   "source": [
    "It's not surprising that these words did not yield useful results, as they're very common."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e7ce2",
   "metadata": {},
   "source": [
    "## All words model\n",
    "\n",
    "We'll see if anything changes when we feed the model all possible tokens rather than just the 10 most common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0373bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "all_words_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Plurality:', plurality_cv,\n",
    "      '\\nBaseline: ',baseline_cv,\n",
    "      '\\nAll Words:',all_words_cv\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee77a3c",
   "metadata": {},
   "source": [
    "This did indeed improve the accuracy.\n",
    "\n",
    "Let's see what words it scored as most useful by their tfidf scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d57a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the word list from this vectorizer with new index\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "# create array of the indices of the feature_names array, ordered by tfidf score\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "# create frequency distribution (dictionary) of 1-grams\n",
    "all_words_freq_dist = FreqDist(X_train['text_tokenized'].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the words with the top 10 tfidf values, and their tfidf values\n",
    "print('10 most useful words', '\\n\\ntfidf | frequency | word\\n')\n",
    "for n in range(-1,-11,-1):\n",
    "    index = sorted_tfidf_index[n]\n",
    "    print(\n",
    "        round(X_train_vectorized.max(0).toarray()[0][index],4),\n",
    "        all_words_freq_dist[feature_names[index]],\n",
    "        feature_names[index]\n",
    "         )\n",
    "# show the words with the bottom 5 tfidf values, and their tfidf values\n",
    "print('\\n5 least useful words', '\\n\\ntfidf | frequency | word\\n')\n",
    "for n in range(5):\n",
    "    index = sorted_tfidf_index[n]\n",
    "    print(\n",
    "        round(X_train_vectorized.max(0).toarray()[0][index],4),\n",
    "        all_words_freq_dist[feature_names[index]],\n",
    "        feature_names[index]\n",
    "         )      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9919ec2e",
   "metadata": {},
   "source": [
    "## Stemmed and lemmatized model\n",
    "\n",
    "Now we'll stem and lemmatize the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a stemmer and lemmatizer\n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd978d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function that tokenizes, stems and lemmatizes a document\n",
    "def stem_and_lemmatize_and_tokenize(document):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    return [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb451175",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a column whose values are a list of stemmed and lemmatized words\n",
    "X_train.loc[:, 'stem_lemma_list'] = X_train.loc[:, 'text'].apply(stem_and_lemmatize_and_tokenize)\n",
    "# make vocabulary from set of words\n",
    "vocab_stemmed_and_lemmatized = set(X_train['stem_lemma_list'].explode())\n",
    "# compare vocabulary sizes\n",
    "print('Size of raw vocabulary:                   ', len(vocab_raw),\n",
    "      '\\nSize of stemmed and lemmatized vocabulary:', len(vocab_stemmed_and_lemmatized)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4567f7d",
   "metadata": {},
   "source": [
    "The stemmed and lemmatized vocabulary is significantly smaller. We can now investigate whether this has introduced new duplicate values for the stemmed and lemmatized text.\n",
    "\n",
    "The feature we just created contains *lists* of lemmatized words. In order to find out whether we have duplicates, we'll need to reassemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3b60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.duplicated(subset=['stem_lemma_list']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cea403",
   "metadata": {},
   "source": [
    "# WHAT TO DO ABOUT NEW DUPLICATES?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b06177",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer = stem_and_lemmatize_and_tokenize\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "stemmed_and_lemmatized_words_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Baseline:                    ',baseline_cv,\n",
    "      '\\nAll Words:                   ',all_words_cv,\n",
    "      '\\nStemmed and Lemmatized Words:', stemmed_and_lemmatized_words_cv\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3328a3",
   "metadata": {},
   "source": [
    "It appears that feeding it lemmatized words just slightly decreased accuracy.\n",
    "\n",
    "We know that the top 10 baseline model suffered because the most frequent words were useless. Let's experiment with a maximum document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bccac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df_vals = np.linspace(1/100, 1, 100)\n",
    "max_df_scores = []\n",
    "for max_df in max_df_vals:\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_df = max_df\n",
    "    )\n",
    "\n",
    "    X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "    max_df_scores.append(round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d7fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(max_df_vals, max_df_scores)\n",
    "plt.ticklabel_format(axis='both', style='plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer = stem_and_lemmatize_and_tokenize,\n",
    "    min_df = 2,\n",
    "    max_df = .02\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "stemmed_and_lemmatized_words_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Plurality:                   ', plurality_cv,\n",
    "      '\\nStemmed and Lemmatized Words:', stemmed_and_lemmatized_words_cv\n",
    "     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
