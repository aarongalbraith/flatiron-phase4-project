{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f413e3ce",
   "metadata": {},
   "source": [
    "# This version scores the basic models:\n",
    "- baseline (top ten)\n",
    "- all words\n",
    "- stemmed words\n",
    "- lemmatized words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ebc87",
   "metadata": {},
   "source": [
    "import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b787f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from operator import itemgetter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7930e57",
   "metadata": {},
   "source": [
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d220fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/df.csv')\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2351148",
   "metadata": {},
   "source": [
    "could do more here to explore the neither and both values\n",
    "\n",
    "move on to language processing\n",
    "\n",
    "train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d37c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['text'].to_frame(), df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11408c1a",
   "metadata": {},
   "source": [
    "add label to X_train for research purposes .. obviously don't include this in the model\n",
    "\n",
    "reset index to anticipate future problems ... or not reset the index???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fe6e0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-0cb08c382a27>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:, 'label'] = [y_train.loc[val] for val in X_train.index]\n"
     ]
    }
   ],
   "source": [
    "X_train.loc[:, 'label'] = [y_train.loc[val] for val in X_train.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cadb14",
   "metadata": {},
   "source": [
    "perfunctory exploring should happen here\n",
    "\n",
    "top ten visualizations for pos. and non-pos.\n",
    "\n",
    "size of vocabulary\n",
    "\n",
    "more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "142ee578",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "tokenizer = RegexpTokenizer(basic_token_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bde293ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of raw vocabulary: 8876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-5ba9b3884ac5>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:, 'text_tokenized'] = X_train['text'].apply(tokenizer.tokenize)\n"
     ]
    }
   ],
   "source": [
    "X_train.loc[:, 'text_tokenized'] = X_train['text'].apply(tokenizer.tokenize)\n",
    "vocab_raw = set(X_train['text_tokenized'].explode())\n",
    "print('Size of raw vocabulary:', len(vocab_raw))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac8ce13",
   "metadata": {},
   "source": [
    "gonna need naive bayes, might not do any other models (markov, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f660a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa82be",
   "metadata": {},
   "source": [
    "look at plurality winner to see score to beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "812fd0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    0.672366\n",
       "1    0.327634\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plurality_cv = round(y_train.value_counts(normalize=True)[0],4)\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfbfbf9",
   "metadata": {},
   "source": [
    "first model, just ten features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6780b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plurality: 0.6724 \n",
      "Baseline:  0.6724\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features = 10\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "baseline_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Plurality:', plurality_cv,\n",
    "      '\\nBaseline: ',baseline_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553da123",
   "metadata": {},
   "source": [
    "an absolutely miniscule improvement\n",
    "\n",
    "let's try all words, not just max_features = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45ce3edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plurality: 0.6724 \n",
      "Baseline:  0.6724 \n",
      "All Words: 0.7005\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "all_words_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Plurality:', plurality_cv,\n",
    "      '\\nBaseline: ', baseline_cv,\n",
    "      '\\nAll Words:', all_words_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878b30c",
   "metadata": {},
   "source": [
    "an actual improvement\n",
    "\n",
    "let's look at which 10 terms were least and most associated with positive sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0560f",
   "metadata": {},
   "source": [
    "we can and will explore stopwords, but it seems clear we can stem or lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4477862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "def stem_and_tokenize(document):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac2154",
   "metadata": {},
   "source": [
    "create stemmed vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2184aef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of raw vocabulary:     8876\n",
      "Size of stemmed vocabulary: 7016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-918cbb3bc428>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:, 'text_stemmed'] = X_train.loc[:, 'text'].apply(stem_and_tokenize)\n"
     ]
    }
   ],
   "source": [
    "X_train.loc[:, 'text_stemmed'] = X_train.loc[:, 'text'].apply(stem_and_tokenize)\n",
    "vocab_stemmed = set(X_train['text_stemmed'].explode())\n",
    "print('Size of raw vocabulary:    ', len(vocab_raw))\n",
    "print('Size of stemmed vocabulary:', len(vocab_stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84c1224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plurality:     0.6724 \n",
      "Baseline:      0.6724 \n",
      "All Words:     0.7005 \n",
      "Stemmed Words: 0.6995\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer = stem_and_tokenize\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "stemmed_words_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Plurality:    ', plurality_cv,\n",
    "      '\\nBaseline:     ', baseline_cv,\n",
    "      '\\nAll Words:    ', all_words_cv,\n",
    "      '\\nStemmed Words:', stemmed_words_cv\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d9392",
   "metadata": {},
   "source": [
    "Stemming is worse by about one tenth of a percent\n",
    "\n",
    "top 10 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "372c2d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_and_tokenize(document):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db125f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of raw vocabulary:        8876 \n",
      "Size of stemmed vocabulary:    7016 \n",
      "Size of lemmatized vocabulary: 8208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-89c3d8203887>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:, 'text_lemmatized'] = X_train.loc[:, 'text'].apply(lemmatize_and_tokenize)\n"
     ]
    }
   ],
   "source": [
    "X_train.loc[:, 'text_lemmatized'] = X_train.loc[:, 'text'].apply(lemmatize_and_tokenize)\n",
    "vocab_lemmatized = set(X_train['text_lemmatized'].explode())\n",
    "print('Size of raw vocabulary:       ', len(vocab_raw),\n",
    "      '\\nSize of stemmed vocabulary:   ', len(vocab_stemmed),\n",
    "      '\\nSize of lemmatized vocabulary:', len(vocab_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "813279ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plurality:        0.6724 \n",
      "All Words:        0.7005 \n",
      "Stemmed Words:    0.6995 \n",
      "Lemmatized Words: 0.6984\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer = lemmatize_and_tokenize\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "lemmatized_words_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Plurality:       ', plurality_cv,\n",
    "      '\\nAll Words:       ', all_words_cv,\n",
    "      '\\nStemmed Words:   ', stemmed_words_cv,\n",
    "      '\\nLemmatized Words:', lemmatized_words_cv\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4b5a2",
   "metadata": {},
   "source": [
    "lemmatizing makes it worse by another tenth of a percent\n",
    "\n",
    "look at top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1decf1a5",
   "metadata": {},
   "source": [
    "top ten only looks at top ten most frequent, so this is useless until you give it a stop words list, probably not useful at all\n",
    "\n",
    "now i have a way to find the top and bottom tfidf scores, i need to take this one step further to see how frequent those terms are, this will help me to decide to ignore n > 4 for example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ffab3",
   "metadata": {},
   "source": [
    "getting absolutely nowhere"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
