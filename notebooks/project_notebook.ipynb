{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a10002e",
   "metadata": {},
   "source": [
    "import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e51cde73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2beb9f",
   "metadata": {},
   "source": [
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a957ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/tweets.csv', encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a3545",
   "metadata": {},
   "source": [
    "explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c41f469",
   "metadata": {},
   "source": [
    "rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cda3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'tweet_text': 'text',\n",
    "                   'emotion_in_tweet_is_directed_at': 'company',\n",
    "                   'is_there_an_emotion_directed_at_a_brand_or_product': 'sentiment'},\n",
    "          inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbaf229",
   "metadata": {},
   "source": [
    "look at missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5d1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.text.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3818c618",
   "metadata": {},
   "source": [
    "can't do anything without the text of the tweet, so drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c2cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee51222",
   "metadata": {},
   "source": [
    "check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98383be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f59d7c",
   "metadata": {},
   "source": [
    "drop duplicates, just text, doesn't matter if same text with different sentiment, etc. (still drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c29d4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de13a97b",
   "metadata": {},
   "source": [
    "begin editing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da09d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c2e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ff2a4",
   "metadata": {},
   "source": [
    "edit, simplify, rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bb11423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].replace({'No emotion toward brand or product': 'neutral',\n",
    "                         'Positive emotion': 'positive',\n",
    "                         'Negative emotion': 'negative',\n",
    "                         \"I can't tell\": 'other'\n",
    "                        }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff251778",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['company'].replace(['iPad', 'Apple', 'iPad or iPhone App', 'iPhone', 'Other Apple product or service'], 'apple',\n",
    "                     inplace=True)\n",
    "df['company'].replace(['Google', 'Other Google product or service', 'Android App', 'Android'], 'google',\n",
    "                     inplace=True)\n",
    "df['company'].fillna('other',\n",
    "                    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e081850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.company.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724cff98",
   "metadata": {},
   "source": [
    "deal with missing company after tokenizing, etc. follow code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5389a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_words = ['ipad', 'apple', 'iphone', 'itunes', 'ipad2']\n",
    "google_words = ['google', 'android', 'blogger']\n",
    "\n",
    "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "tokenizer = RegexpTokenizer(basic_token_pattern)\n",
    "\n",
    "def company_fix(text, company):\n",
    "    if company != 'other':\n",
    "        return company\n",
    "    else:\n",
    "        apple, google = False, False\n",
    "        text_tokenized = tokenizer.tokenize(text)\n",
    "        for word in apple_words:\n",
    "            if word in text_tokenized:\n",
    "                apple = True\n",
    "                break\n",
    "        for word in google_words:\n",
    "            if word in text_tokenized:\n",
    "                google = True\n",
    "                break\n",
    "        if apple & ~google:\n",
    "            return 'apple'\n",
    "        elif google & ~apple:\n",
    "            return 'google'\n",
    "        elif apple & google:\n",
    "            return 'both'\n",
    "        else:\n",
    "            return 'neither'\n",
    "\n",
    "df['company'] = df.apply(lambda x: company_fix(x.text, x.company), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e23ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.company.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c24608d",
   "metadata": {},
   "source": [
    "could do more here to explore the neither and both values\n",
    "\n",
    "move on to language processing\n",
    "\n",
    "train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64c7b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['text'].to_frame(), df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede12fe5",
   "metadata": {},
   "source": [
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec0e1b80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-1675f3322f1d>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['text_tokenized'] = X_train['text'].apply(tokenizer.tokenize)\n"
     ]
    }
   ],
   "source": [
    "X_train['text_tokenized'] = X_train['text'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e01a6",
   "metadata": {},
   "source": [
    "explore complete vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b63083b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8876"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "for row in X_train['text_tokenized']:\n",
    "    vocabulary.update(set(row))\n",
    "    \n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f8e819e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '00am',\n",
       " '00pm',\n",
       " '01am',\n",
       " '03',\n",
       " '0310apple',\n",
       " '06',\n",
       " '08',\n",
       " '10',\n",
       " '100',\n",
       " '100s',\n",
       " '100tc',\n",
       " '101',\n",
       " '106',\n",
       " '10am',\n",
       " '10k',\n",
       " '10x',\n",
       " '10x2',\n",
       " '11',\n",
       " '1100',\n",
       " '1154',\n",
       " '11am',\n",
       " '11ntc',\n",
       " '11p',\n",
       " '11pm',\n",
       " '11th',\n",
       " '12',\n",
       " '120',\n",
       " '1223',\n",
       " '125',\n",
       " '128',\n",
       " '12am',\n",
       " '12b',\n",
       " '12bn',\n",
       " '12th',\n",
       " '13',\n",
       " '130',\n",
       " '1300',\n",
       " '14',\n",
       " '1406',\n",
       " '1408',\n",
       " '141164002609303',\n",
       " '1413',\n",
       " '1415',\n",
       " '1422',\n",
       " '1443',\n",
       " '14th',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '150m',\n",
       " '150mm',\n",
       " '157',\n",
       " '15am',\n",
       " '15k',\n",
       " '15pm',\n",
       " '15slides',\n",
       " '16',\n",
       " '16162',\n",
       " '165',\n",
       " '169',\n",
       " '16gb',\n",
       " '16mins',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1985',\n",
       " '1986',\n",
       " '1990style',\n",
       " '1991',\n",
       " '1k',\n",
       " '1m',\n",
       " '1pm',\n",
       " '1s',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2009',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '206',\n",
       " '206k',\n",
       " '20s',\n",
       " '21',\n",
       " '210',\n",
       " '2100',\n",
       " '214',\n",
       " '22',\n",
       " '22sxsw',\n",
       " '23',\n",
       " '230',\n",
       " '24',\n",
       " '24587',\n",
       " '25',\n",
       " '250',\n",
       " '250k',\n",
       " '26svo3m',\n",
       " '27',\n",
       " '270',\n",
       " '285',\n",
       " '29',\n",
       " '2b',\n",
       " '2day',\n",
       " '2g',\n",
       " '2h',\n",
       " '2honor',\n",
       " '2moro',\n",
       " '2nd',\n",
       " '2nite',\n",
       " '2rd',\n",
       " '2s',\n",
       " '2wks',\n",
       " '2x',\n",
       " '2yrs',\n",
       " '2åê',\n",
       " '30',\n",
       " '300',\n",
       " '30a',\n",
       " '30am',\n",
       " '30k',\n",
       " '30min',\n",
       " '30mio',\n",
       " '30pm',\n",
       " '30th',\n",
       " '31',\n",
       " '313',\n",
       " '32',\n",
       " '32000',\n",
       " '32g',\n",
       " '32gb',\n",
       " '33',\n",
       " '330',\n",
       " '330pm',\n",
       " '35',\n",
       " '350',\n",
       " '36',\n",
       " '360idev',\n",
       " '365plusmedia',\n",
       " '37',\n",
       " '39',\n",
       " '3am',\n",
       " '3blks',\n",
       " '3d',\n",
       " '3g',\n",
       " '3gs',\n",
       " '3k',\n",
       " '3pm',\n",
       " '3rd',\n",
       " '3x',\n",
       " '40',\n",
       " '400',\n",
       " '40min',\n",
       " '41',\n",
       " '420',\n",
       " '43',\n",
       " '437',\n",
       " '44',\n",
       " '45',\n",
       " '450',\n",
       " '457',\n",
       " '45am',\n",
       " '45pm',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '49n4m',\n",
       " '4am',\n",
       " '4chan',\n",
       " '4chan4eva',\n",
       " '4g',\n",
       " '4hb',\n",
       " '4nqv92l',\n",
       " '4pm',\n",
       " '4sq',\n",
       " '4sq3',\n",
       " '4square',\n",
       " '4th',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '504',\n",
       " '512',\n",
       " '54',\n",
       " '55',\n",
       " '58',\n",
       " '59',\n",
       " '59a',\n",
       " '59p',\n",
       " '59pm',\n",
       " '5am',\n",
       " '5min',\n",
       " '5pm',\n",
       " '5th',\n",
       " '60',\n",
       " '600',\n",
       " '60secondcrush',\n",
       " '615ab',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '64g',\n",
       " '64gb',\n",
       " '64gig',\n",
       " '64mb',\n",
       " '65',\n",
       " '66',\n",
       " '68',\n",
       " '681',\n",
       " '69',\n",
       " '699',\n",
       " '6gjmypj',\n",
       " '6pm',\n",
       " '6th',\n",
       " '6thish',\n",
       " '6thst',\n",
       " '6thstreet',\n",
       " '70',\n",
       " '72296',\n",
       " '73',\n",
       " '7322',\n",
       " '75',\n",
       " '78',\n",
       " '7pm',\n",
       " '7th',\n",
       " '80',\n",
       " '800',\n",
       " '80s',\n",
       " '81',\n",
       " '812',\n",
       " '82',\n",
       " '829',\n",
       " '83323324',\n",
       " '83323414',\n",
       " '83881586',\n",
       " '85',\n",
       " '86',\n",
       " '88',\n",
       " '8800',\n",
       " '89',\n",
       " '8a',\n",
       " '8am',\n",
       " '8p',\n",
       " '8pm',\n",
       " '8th',\n",
       " '90',\n",
       " '900',\n",
       " '90999',\n",
       " '911',\n",
       " '911tweets',\n",
       " '930a',\n",
       " '95',\n",
       " '96',\n",
       " '967',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '99å',\n",
       " '9abc',\n",
       " '9th',\n",
       " '__',\n",
       " '___',\n",
       " '_____',\n",
       " '______',\n",
       " '_______',\n",
       " '_and_',\n",
       " '_really_',\n",
       " '_µ',\n",
       " '_¼',\n",
       " '_ã',\n",
       " '_ô',\n",
       " 'a11y2go',\n",
       " 'a3xvwc6',\n",
       " 'aapl',\n",
       " 'aaron',\n",
       " 'aarpbulletin',\n",
       " 'ab',\n",
       " 'abacus',\n",
       " 'abandoned',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abnormal',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absolutely',\n",
       " 'absolutley',\n",
       " 'abt',\n",
       " 'abuzz',\n",
       " 'academy',\n",
       " 'acc',\n",
       " 'accelerater',\n",
       " 'acceleration',\n",
       " 'accelerator',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessories',\n",
       " 'accessory',\n",
       " 'accidentally',\n",
       " 'accommodate',\n",
       " 'accompanied',\n",
       " 'according',\n",
       " 'accordion',\n",
       " 'account',\n",
       " 'accounts',\n",
       " 'accuracy',\n",
       " 'acerbic',\n",
       " 'achieve',\n",
       " 'achievement',\n",
       " 'ackward',\n",
       " 'aclu',\n",
       " 'aclus',\n",
       " 'acoustic',\n",
       " 'acquired',\n",
       " 'acquisition',\n",
       " 'across',\n",
       " 'acrosse',\n",
       " 'act',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actionable',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'activity',\n",
       " 'actsofsharing',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'adapt',\n",
       " 'adapters',\n",
       " 'adapting',\n",
       " 'adaptive',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addictedtotheinterwebs',\n",
       " 'addictive',\n",
       " 'addicts',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'addr',\n",
       " 'address',\n",
       " 'adds',\n",
       " 'adele',\n",
       " 'adfonic',\n",
       " 'adi',\n",
       " 'adloopz',\n",
       " 'admired',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admitting',\n",
       " 'admob',\n",
       " 'ado',\n",
       " 'adobe',\n",
       " 'adopter',\n",
       " 'adopters',\n",
       " 'adoption',\n",
       " 'adpeopleproblems',\n",
       " 'ads',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'advent',\n",
       " 'adventure',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advil',\n",
       " 'advisory',\n",
       " 'adwords',\n",
       " 'ae',\n",
       " 'aesthetic',\n",
       " 'afar',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affiliated',\n",
       " 'affirmative',\n",
       " 'afford',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'afteward',\n",
       " 'aftrnoon',\n",
       " 'again',\n",
       " 'against',\n",
       " 'agchat',\n",
       " 'age',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agents',\n",
       " 'aggregator',\n",
       " 'agileagency',\n",
       " 'agility',\n",
       " 'agnerd',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahead',\n",
       " 'ahem',\n",
       " 'ahh',\n",
       " 'ahing',\n",
       " 'ahold',\n",
       " 'ahoy',\n",
       " 'ai',\n",
       " 'aicn',\n",
       " 'aid',\n",
       " 'aiding',\n",
       " 'aight',\n",
       " 'aim',\n",
       " 'aims',\n",
       " 'ain',\n",
       " 'air',\n",
       " 'airline',\n",
       " 'airlines',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'airports',\n",
       " 'airs',\n",
       " 'aisle',\n",
       " 'ajax',\n",
       " 'ajs2011',\n",
       " 'aka',\n",
       " 'akabuzz',\n",
       " 'akhirnya',\n",
       " 'akqa',\n",
       " 'akqas',\n",
       " 'al',\n",
       " 'ala',\n",
       " 'alacagimiz',\n",
       " 'alamo',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alarms',\n",
       " 'albany',\n",
       " 'album',\n",
       " 'alcohol',\n",
       " 'alcoholics',\n",
       " 'alert',\n",
       " 'alerts',\n",
       " 'alex',\n",
       " 'algorithm',\n",
       " 'alice',\n",
       " 'alignd',\n",
       " 'aligned',\n",
       " 'alisa',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allcomfortfooddiet',\n",
       " 'alley',\n",
       " 'allhat',\n",
       " 'allhat3',\n",
       " 'allow',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'allthingsd',\n",
       " 'almedia',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'alpha',\n",
       " 'alphabetically',\n",
       " 'alphagraphics',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alternate',\n",
       " 'alternative',\n",
       " 'although',\n",
       " 'altimeter',\n",
       " 'alto',\n",
       " 'alumni',\n",
       " 'always',\n",
       " 'alwayshavingtoplugin',\n",
       " 'am',\n",
       " 'amalgamation',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazon',\n",
       " 'ambassador',\n",
       " 'amble',\n",
       " 'amen',\n",
       " 'amer',\n",
       " 'america',\n",
       " 'american',\n",
       " 'american_statesman',\n",
       " 'americans',\n",
       " 'amex',\n",
       " 'ami',\n",
       " 'amid',\n",
       " 'amigos',\n",
       " 'amiss',\n",
       " 'among',\n",
       " 'amoral',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'ample',\n",
       " 'amplifiedlife',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'analogy',\n",
       " 'analytics',\n",
       " 'anatomy',\n",
       " 'and',\n",
       " 'andoid',\n",
       " 'andrew',\n",
       " 'andriod',\n",
       " 'android',\n",
       " 'androiddev',\n",
       " 'androidhig',\n",
       " 'androidsxsw',\n",
       " 'andy',\n",
       " 'angel',\n",
       " 'angry',\n",
       " 'angrybirds',\n",
       " 'animation',\n",
       " 'anna',\n",
       " 'anniemal',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announcements',\n",
       " 'announces',\n",
       " 'announcing',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annuncia',\n",
       " 'anonymity',\n",
       " 'anonymous',\n",
       " 'anoth',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'answr',\n",
       " 'anti',\n",
       " 'anticipate',\n",
       " 'anticipation',\n",
       " 'antigov',\n",
       " 'antique',\n",
       " 'antonio',\n",
       " 'antwoord',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anybodywanttobuymeanipad2',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyones',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'anz',\n",
       " 'aol',\n",
       " 'aos',\n",
       " 'ap',\n",
       " 'apaan',\n",
       " 'apac',\n",
       " 'apart',\n",
       " 'apartmovie',\n",
       " 'ape',\n",
       " 'aphone',\n",
       " 'api',\n",
       " 'apis',\n",
       " 'app',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appcircus',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appeared',\n",
       " 'appears',\n",
       " 'appen',\n",
       " 'applauds',\n",
       " 'applause',\n",
       " 'apple',\n",
       " 'apple2',\n",
       " 'apple_apple',\n",
       " 'apple_store',\n",
       " 'appleapple',\n",
       " 'appleatxdt',\n",
       " 'apples',\n",
       " 'applestore',\n",
       " 'applesxsw',\n",
       " 'appletakingoverworld',\n",
       " 'appletogo',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'appolicious',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciation',\n",
       " 'approach',\n",
       " 'approaches',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'appropriately',\n",
       " 'approval',\n",
       " 'approved',\n",
       " 'approves',\n",
       " 'apps',\n",
       " 'appsavvy',\n",
       " 'appstore',\n",
       " 'apptastic',\n",
       " 'apptrophy',\n",
       " 'appy',\n",
       " 'aproveitem',\n",
       " 'aps',\n",
       " 'apt',\n",
       " 'apture',\n",
       " 'aquent',\n",
       " 'ar',\n",
       " 'arab',\n",
       " 'arabspring',\n",
       " 'arboretum',\n",
       " 'arcade',\n",
       " 'archforhumanity',\n",
       " 'archive',\n",
       " 'arctic',\n",
       " 'arduino',\n",
       " 'are',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'aren',\n",
       " 'arg',\n",
       " 'argh',\n",
       " 'argues',\n",
       " 'argument',\n",
       " 'aristotle',\n",
       " 'arm',\n",
       " 'armadillo',\n",
       " 'armed',\n",
       " 'aron',\n",
       " 'around',\n",
       " 'arrested',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'arrivo',\n",
       " 'arrow',\n",
       " 'arsenal',\n",
       " 'arsense',\n",
       " 'arsxsw',\n",
       " 'art',\n",
       " 'artallaround',\n",
       " 'arthaus',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'articulate',\n",
       " 'articulating',\n",
       " 'artifacts',\n",
       " 'artificial',\n",
       " 'artikelen',\n",
       " 'artist',\n",
       " 'artists',\n",
       " 'artwork',\n",
       " 'artworks',\n",
       " 'arw',\n",
       " 'arwords',\n",
       " 'as',\n",
       " 'asap',\n",
       " 'asd',\n",
       " 'asddieu',\n",
       " 'ask',\n",
       " 'askd',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'aspengrove',\n",
       " 'ass',\n",
       " 'asserted',\n",
       " 'asset',\n",
       " 'assets',\n",
       " 'assignment',\n",
       " 'assistant',\n",
       " 'assisted',\n",
       " 'assistivetech',\n",
       " 'asst',\n",
       " 'assume',\n",
       " 'assumes',\n",
       " 'at',\n",
       " 'at038t',\n",
       " 'atari',\n",
       " 'ate',\n",
       " 'atl',\n",
       " 'atleast',\n",
       " 'atms',\n",
       " 'atomic',\n",
       " 'atrix',\n",
       " 'att',\n",
       " 'attached',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attendance',\n",
       " 'attended',\n",
       " 'attendees',\n",
       " 'attendence',\n",
       " 'attending',\n",
       " 'attention',\n",
       " 'attn',\n",
       " 'attracted',\n",
       " 'attracting',\n",
       " 'attractive',\n",
       " 'attsxsw',\n",
       " 'attys',\n",
       " 'atv',\n",
       " 'atx',\n",
       " 'atzip',\n",
       " 'atåê',\n",
       " 'au',\n",
       " 'audi',\n",
       " 'audience',\n",
       " 'audio',\n",
       " 'audioboo',\n",
       " 'auerbach',\n",
       " 'aug',\n",
       " 'augcomm',\n",
       " 'augmented',\n",
       " 'augmentedreality',\n",
       " 'aun',\n",
       " 'auntie',\n",
       " 'aus',\n",
       " 'austin',\n",
       " 'austinbusiness',\n",
       " 'austincrowd',\n",
       " 'austinites',\n",
       " 'austinl',\n",
       " 'austins',\n",
       " 'austintx',\n",
       " 'australian',\n",
       " 'ausxsw',\n",
       " 'auth',\n",
       " 'authenticator',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'authoritzed',\n",
       " 'authorization',\n",
       " 'authors',\n",
       " 'auto',\n",
       " 'autocorrect',\n",
       " 'autocorrected',\n",
       " 'autocorrects',\n",
       " 'autodial',\n",
       " 'autograph',\n",
       " 'automated',\n",
       " 'automatic',\n",
       " 'automatically',\n",
       " 'automation',\n",
       " 'autonomous',\n",
       " 'avail',\n",
       " 'availability',\n",
       " 'available',\n",
       " 'ave',\n",
       " 'avenue',\n",
       " 'average',\n",
       " 'averages',\n",
       " 'avoid',\n",
       " 'avoided',\n",
       " 'avoiding',\n",
       " 'avro',\n",
       " 'aw',\n",
       " 'awa50',\n",
       " 'awaiting',\n",
       " 'awaits',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'awards',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awesome',\n",
       " 'awesomely',\n",
       " 'awesomeness',\n",
       " 'awesometiming',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awww',\n",
       " 'awwww',\n",
       " 'axsure',\n",
       " 'axzwxb',\n",
       " 'ay',\n",
       " 'azure',\n",
       " 'b4',\n",
       " 'b_social',\n",
       " 'baaah',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'babyheadphones',\n",
       " 'back',\n",
       " 'background',\n",
       " 'backinbusiness',\n",
       " 'backlight',\n",
       " 'backpack',\n",
       " 'backpacking',\n",
       " 'backpages',\n",
       " 'backup',\n",
       " 'backupify',\n",
       " 'backyard',\n",
       " 'bad',\n",
       " 'badass',\n",
       " 'badform',\n",
       " 'badge',\n",
       " 'badgeless',\n",
       " 'badger',\n",
       " 'badges',\n",
       " 'bag',\n",
       " 'bags',\n",
       " 'bah',\n",
       " 'bahahahaha',\n",
       " 'bahrain',\n",
       " 'bail',\n",
       " 'bait',\n",
       " 'bajillions',\n",
       " 'balance',\n",
       " 'balckberries',\n",
       " 'balcony',\n",
       " 'ball',\n",
       " 'baller',\n",
       " 'ballinonabudget',\n",
       " 'ballrm',\n",
       " 'ballroom',\n",
       " 'ballroomd',\n",
       " 'ballrooms',\n",
       " 'balsamic',\n",
       " 'banality',\n",
       " 'banana',\n",
       " 'band',\n",
       " 'bandcamp',\n",
       " 'bands',\n",
       " 'bandwaggoners',\n",
       " 'bandwagon',\n",
       " 'bang',\n",
       " 'banged',\n",
       " 'bank',\n",
       " 'bankers',\n",
       " 'banking',\n",
       " 'bankinnovate',\n",
       " 'bankinnovation',\n",
       " 'banks',\n",
       " 'bar',\n",
       " 'baracades',\n",
       " 'barbarian',\n",
       " 'barcode',\n",
       " 'barely',\n",
       " 'bargain',\n",
       " 'barriers',\n",
       " 'barring',\n",
       " 'barroom',\n",
       " 'barry',\n",
       " 'barry_diller',\n",
       " 'barrydiller',\n",
       " 'bars',\n",
       " 'barton',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'basecamp',\n",
       " 'based',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basics',\n",
       " 'basis',\n",
       " 'basket',\n",
       " 'bass',\n",
       " 'bastards',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'bathroom',\n",
       " 'batshit',\n",
       " 'batt',\n",
       " 'battelle',\n",
       " 'batteries',\n",
       " 'battery',\n",
       " 'batterykiller',\n",
       " 'battle',\n",
       " 'battledecks',\n",
       " 'battlela',\n",
       " 'battles',\n",
       " 'bavc',\n",
       " 'bavcid',\n",
       " 'bawling',\n",
       " 'bay',\n",
       " 'bb',\n",
       " 'bberry',\n",
       " 'bbq',\n",
       " 'bc',\n",
       " 'bday',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beans',\n",
       " 'bear',\n",
       " 'beard',\n",
       " 'bearded',\n",
       " 'beards',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'beating',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'became',\n",
       " 'because',\n",
       " 'beckley',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'bed',\n",
       " 'beechwood',\n",
       " 'beef',\n",
       " 'been',\n",
       " 'beer',\n",
       " 'beers',\n",
       " 'bees',\n",
       " 'beevil',\n",
       " 'before',\n",
       " 'beforetwitter',\n",
       " 'begger',\n",
       " 'begging',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'begins',\n",
       " 'begun',\n",
       " 'behave',\n",
       " 'behaving',\n",
       " 'behavior',\n",
       " 'behaviour',\n",
       " 'behind',\n",
       " 'beiber',\n",
       " 'being',\n",
       " 'belgiums',\n",
       " 'belies',\n",
       " 'believe',\n",
       " 'believes',\n",
       " 'belinsky',\n",
       " 'belli',\n",
       " 'belly',\n",
       " 'belong',\n",
       " 'belt',\n",
       " 'beluga',\n",
       " 'bemyneighbor',\n",
       " 'ben',\n",
       " 'benefit',\n",
       " 'benieuwd',\n",
       " 'beprepared',\n",
       " 'bereft',\n",
       " 'bergstrom',\n",
       " 'berklee',\n",
       " 'berkowitz',\n",
       " 'bernard',\n",
       " 'bernd',\n",
       " 'berry',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'bestappever',\n",
       " 'bestie',\n",
       " 'bestworstthingever',\n",
       " 'bet',\n",
       " 'beta',\n",
       " 'betainvites',\n",
       " 'better',\n",
       " 'bettercloud',\n",
       " 'bettersearch',\n",
       " 'betterthingstodo',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'beyondwc',\n",
       " 'bff',\n",
       " 'bgr',\n",
       " 'bible',\n",
       " 'bicycle',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bigmistake',\n",
       " 'bike',\n",
       " 'bikes',\n",
       " 'bill',\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186073e",
   "metadata": {},
   "source": [
    "explore top ten frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1401098",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_freq_dist = FreqDist(X_train[\"text_tokenized\"].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "016e3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_top_10(freq_dist, title):\n",
    "    # Extract data for plotting\n",
    "    top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "    tokens = top_10[0]\n",
    "    counts = top_10[1]\n",
    "    \n",
    "    # Set up plot and plot data\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(tokens, counts)\n",
    "    \n",
    "    # Customize plot appearance\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    \n",
    "# visualize_top_10(train_freq_dist, \"Top 10 Most Common Words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e7429e",
   "metadata": {},
   "source": [
    "explore vectorized data with just top ten frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c3b2db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01am</th>\n",
       "      <th>03</th>\n",
       "      <th>0310apple</th>\n",
       "      <th>06</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>ûïwin</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûòand</th>\n",
       "      <th>ûó</th>\n",
       "      <th>ûócan</th>\n",
       "      <th>ûójust</th>\n",
       "      <th>ûólewis</th>\n",
       "      <th>ûólots</th>\n",
       "      <th>ûómy</th>\n",
       "      <th>ûóthe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7247</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7248</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7249</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7250</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7251</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7252 rows × 8000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  000  00am  00pm  01am   03  0310apple   06   08   10  ...  ûïwin  \\\n",
       "0     0.0  0.0   0.0   0.0   0.0  0.0        0.0  0.0  0.0  0.0  ...    0.0   \n",
       "1     0.0  0.0   0.0   0.0   0.0  0.0        0.0  0.0  0.0  0.0  ...    0.0   \n",
       "2     0.0  0.0   0.0   0.0   0.0  0.0        0.0  0.0  0.0  0.0  ...    0.0   \n",
       "3     0.0  0.0   0.0   0.0   0.0  0.0        0.0  0.0  0.0  0.0  ...    0.0   \n",
       "4     0.0  0.0   0.0   0.0   0.0  0.0        0.0  0.0  0.0  0.0  ...    0.0   \n",
       "...   ...  ...   ...   ...   ...  ...        ...  ...  ...  ...  ...    ...   \n",
       "7247  0.0  0.0   0.0   0.0   0.0  0.0        0.0  0.0  0.0  0.0  ...    0.0   \n",
       "7248  0.0  0.0   0.0   0.0   0.0  0.0        0.0  0.0  0.0  0.0  ...    0.0   \n",
       "7249  0.0  0.0   0.0   0.0   0.0  0.0        0.0  0.0  0.0  0.0  ...    0.0   \n",
       "7250  0.0  0.0   0.0   0.0   0.0  0.0        0.0  0.0  0.0  0.0  ...    0.0   \n",
       "7251  0.0  0.0   0.0   0.0   0.0  0.0        0.0  0.0  0.0  0.0  ...    0.0   \n",
       "\n",
       "       ûò  ûòand   ûó  ûócan  ûójust  ûólewis  ûólots  ûómy  ûóthe  \n",
       "0     0.0    0.0  0.0    0.0     0.0      0.0     0.0   0.0    0.0  \n",
       "1     0.0    0.0  0.0    0.0     0.0      0.0     0.0   0.0    0.0  \n",
       "2     0.0    0.0  0.0    0.0     0.0      0.0     0.0   0.0    0.0  \n",
       "3     0.0    0.0  0.0    0.0     0.0      0.0     0.0   0.0    0.0  \n",
       "4     0.0    0.0  0.0    0.0     0.0      0.0     0.0   0.0    0.0  \n",
       "...   ...    ...  ...    ...     ...      ...     ...   ...    ...  \n",
       "7247  0.0    0.0  0.0    0.0     0.0      0.0     0.0   0.0    0.0  \n",
       "7248  0.0    0.0  0.0    0.0     0.0      0.0     0.0   0.0    0.0  \n",
       "7249  0.0    0.0  0.0    0.0     0.0      0.0     0.0   0.0    0.0  \n",
       "7250  0.0    0.0  0.0    0.0     0.0      0.0     0.0   0.0    0.0  \n",
       "7251  0.0    0.0  0.0    0.0     0.0      0.0     0.0   0.0    0.0  \n",
       "\n",
       "[7252 rows x 8000 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=8000)\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42d0bb",
   "metadata": {},
   "source": [
    "perform naive bayes on this vectorized set to make first model and look at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8c099e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d1fbf",
   "metadata": {},
   "source": [
    "below is a failed attempt to make a function that automates the testing, maybe return to this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40157206",
   "metadata": {},
   "outputs": [],
   "source": [
    "## iteration 1\n",
    "def run_test(model, preprocessed_X_train, y_train):\n",
    "    return cross_val_score(model, preprocessed_X_train, y_train).mean()\n",
    "\n",
    "# def run_test(columns = [], model = baseline_model, tfidf = tfidf):\n",
    "    \n",
    "#     X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "    \n",
    "#     preprocessed_X_train = pd.concat([\n",
    "#         X_train_vectorized, X_train[columns]\n",
    "#     ], axis=1)\n",
    "    \n",
    "#     return cross_val_score(model, preprocessed_X_train, y_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171b0b2",
   "metadata": {},
   "source": [
    "here is the traditional way to run the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dee68ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.633480168254949"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_cv = run_test(baseline_model, X_train_vectorized, y_train)\n",
    "baseline_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389bb47",
   "metadata": {},
   "source": [
    "look at plurality winner to see how first model (naive bayes) compares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33f43030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neutral     0.591975\n",
       "positive    0.327634\n",
       "negative    0.062879\n",
       "other       0.017512\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46403585",
   "metadata": {},
   "source": [
    "naive bayes is essentially exactly same as plurality\n",
    "\n",
    "first model is bad\n",
    "\n",
    "introduce and remove stopwords, redo model same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(token_list):\n",
    "    stopwords_removed = [token for token in token_list if token not in stopwords_list]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd6dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"text_without_stopwords\"] = X_train[\"text_tokenized\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c84b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10,\n",
    "    stop_words=stopwords_list\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\n",
    "\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_removed_cv = run_test(baseline_model, X_train_vectorized, y_train)\n",
    "stopwords_removed_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a429ca",
   "metadata": {},
   "source": [
    "still very bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ba4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline:         \", baseline_cv.mean())\n",
    "print(\"Stopwords removed:\", stopwords_removed_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "def stem_and_tokenize(document):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cfe9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_stopwords = [stemmer.stem(word) for word in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10,\n",
    "    stop_words=stemmed_stopwords,\n",
    "    tokenizer=stem_and_tokenize\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\n",
    "\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3cb81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_cv = run_test(baseline_model, X_train_vectorized, y_train)\n",
    "stemmed_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stopwords removed:\", stopwords_removed_cv.mean())\n",
    "print(\"Stemmed:          \", stemmed_cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e079d0",
   "metadata": {},
   "source": [
    "getting absolutely nowhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ed003",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['num_words'] = X_train['text_tokenized'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0df98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"num_sentences\"] = X_train[\"text\"].apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ffe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"label\"] = [y_train[val] for val in X_train.index]\n",
    "\n",
    "def plot_words(column, title):\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 9))\n",
    "    fig.set_tight_layout(True)\n",
    "    gs = fig.add_gridspec(2, 2)\n",
    "    ax1 = fig.add_subplot(gs[0, :1])\n",
    "    ax2 = fig.add_subplot(gs[0, 1:2])\n",
    "    ax3 = fig.add_subplot(gs[1, :1])\n",
    "    ax4 = fig.add_subplot(gs[1, 1:2])\n",
    "\n",
    "    axes = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "    for index, category in enumerate(y_train.unique()):\n",
    "\n",
    "        all_words = X_train[X_train[\"label\"] == category][column].explode()\n",
    "        freq_dist = FreqDist(all_words)\n",
    "        top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "        tokens = top_10[0]\n",
    "        counts = top_10[1]\n",
    "\n",
    "        ax = axes[index]\n",
    "        ax.bar(tokens, counts)\n",
    "\n",
    "        ax.set_title(f\"{title} {category}\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax.tick_params(axis=\"x\", rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7708e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_words('text_without_stopwords', 'fuckoff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd931a8",
   "metadata": {},
   "source": [
    "brainstorm feature engineering:\n",
    "\n",
    "- whether / how many times the product is mentioned\n",
    "- number of words\n",
    "- contains an emoji\n",
    "- look at bigrams (37.08)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d17a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
