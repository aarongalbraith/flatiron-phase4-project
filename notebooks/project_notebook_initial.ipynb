{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a5da7a",
   "metadata": {},
   "source": [
    "import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010a0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2fa195bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1eb457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from operator import itemgetter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af64a4",
   "metadata": {},
   "source": [
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400968bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/tweets.csv', encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e115419",
   "metadata": {},
   "source": [
    "explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61f98a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80af45a",
   "metadata": {},
   "source": [
    "rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "470eecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'tweet_text': 'text',\n",
    "                   'emotion_in_tweet_is_directed_at': 'company',\n",
    "                   'is_there_an_emotion_directed_at_a_brand_or_product': 'sentiment'},\n",
    "          inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2bbe19",
   "metadata": {},
   "source": [
    "look at missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "611927b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>company</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  text company                           sentiment\n",
       "6  NaN     NaN  No emotion toward brand or product"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.text.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d360c",
   "metadata": {},
   "source": [
    "can't do anything without the text of the tweet, so drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c8d06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c8f81d",
   "metadata": {},
   "source": [
    "check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e48d85d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    9070\n",
       "True       22\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3b2765",
   "metadata": {},
   "source": [
    "drop duplicates, just text, doesn't matter if same text with different sentiment, etc. (still drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a6381cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8787f",
   "metadata": {},
   "source": [
    "edit, simplify, rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a925b71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "No emotion toward brand or product    5372\n",
       "Positive emotion                      2968\n",
       "Negative emotion                       569\n",
       "I can't tell                           156\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ce2151",
   "metadata": {},
   "source": [
    "simplify sentiment into binary, reduce class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe839708",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].replace({'No emotion toward brand or product': 0,\n",
    "                         'Positive emotion': 1,\n",
    "                         'Negative emotion': 0,\n",
    "                         \"I can't tell\": 0\n",
    "                        }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbb2433c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    6097\n",
       "1    2968\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f7976",
   "metadata": {},
   "source": [
    "look at company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "339a7c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company\n",
       "iPad                               943\n",
       "Apple                              659\n",
       "iPad or iPhone App                 469\n",
       "Google                             428\n",
       "iPhone                             296\n",
       "Other Google product or service    293\n",
       "Android App                         80\n",
       "Android                             77\n",
       "Other Apple product or service      35\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.company.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42b502d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['company'].replace(['iPad', 'Apple', 'iPad or iPhone App', 'iPhone', 'Other Apple product or service'], 'apple',\n",
    "                     inplace=True)\n",
    "df['company'].replace(['Google', 'Other Google product or service', 'Android App', 'Android'], 'google',\n",
    "                     inplace=True)\n",
    "df['company'].fillna('other',\n",
    "                    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "670b6734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company\n",
       "other     5785\n",
       "apple     2402\n",
       "google     878\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.company.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2960c122",
   "metadata": {},
   "source": [
    "deal with missing company\n",
    "missing company values are informed by the text, and the text should be all lower case to simplify this\n",
    "no big deal because we want all lower case for train and test anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77073f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2a95a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_words = ['ipad', 'apple', 'iphone', 'itunes', 'ipad2']\n",
    "google_words = ['google', 'android', 'blogger']\n",
    "\n",
    "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "tokenizer = RegexpTokenizer(basic_token_pattern)\n",
    "\n",
    "def company_fix(text, company):\n",
    "    if company != 'other':\n",
    "        return company\n",
    "    else:\n",
    "        apple, google = False, False\n",
    "        text_tokenized = tokenizer.tokenize(text)\n",
    "        for word in apple_words:\n",
    "            if word in text_tokenized:\n",
    "                apple = True\n",
    "                break\n",
    "        for word in google_words:\n",
    "            if word in text_tokenized:\n",
    "                google = True\n",
    "                break\n",
    "        if apple & ~google:\n",
    "            return 'apple'\n",
    "        elif google & ~apple:\n",
    "            return 'google'\n",
    "        elif apple & google:\n",
    "            return 'both'\n",
    "        else:\n",
    "            return 'neither'\n",
    "\n",
    "df['company'] = df.apply(lambda x: company_fix(x.text, x.company), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7577237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company\n",
       "apple      5390\n",
       "google     2783\n",
       "neither     716\n",
       "both        176\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.company.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9295f7b8",
   "metadata": {},
   "source": [
    "could do more here to explore the neither and both values\n",
    "\n",
    "move on to language processing\n",
    "\n",
    "train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b49c5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['text'].to_frame(), df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e0bf9",
   "metadata": {},
   "source": [
    "add label to X_train for research purposes .. obviously don't include this in the model\n",
    "\n",
    "reset index to anticipate future problems ... or not reset the index???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bd2e1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-0cb08c382a27>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:, 'label'] = [y_train.loc[val] for val in X_train.index]\n"
     ]
    }
   ],
   "source": [
    "X_train.loc[:, 'label'] = [y_train.loc[val] for val in X_train.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9244d3b",
   "metadata": {},
   "source": [
    "perfunctory exploring should happen here\n",
    "\n",
    "top ten visualizations for pos. and non-pos.\n",
    "\n",
    "size of vocabulary\n",
    "\n",
    "more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9166e1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of raw vocabulary: 8876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-5ba9b3884ac5>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:, 'text_tokenized'] = X_train['text'].apply(tokenizer.tokenize)\n"
     ]
    }
   ],
   "source": [
    "X_train.loc[:, 'text_tokenized'] = X_train['text'].apply(tokenizer.tokenize)\n",
    "vocab_raw = set(X_train['text_tokenized'].explode())\n",
    "print('Size of raw vocabulary:', len(vocab_raw))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb19d817",
   "metadata": {},
   "source": [
    "gonna need naive bayes, might not do any other models (markov, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7690815",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4087648",
   "metadata": {},
   "source": [
    "look at plurality winner to see score to beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c368993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    0.672366\n",
       "1    0.327634\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plurality_cv = round(y_train.value_counts(normalize=True)[0],4)\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f735c0",
   "metadata": {},
   "source": [
    "first model, just ten features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8265625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plurality: 0.6724 \n",
      "Baseline:  0.6724\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features = 10\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "baseline_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Plurality:', plurality_cv,\n",
    "      '\\nBaseline: ',baseline_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645fea0f",
   "metadata": {},
   "source": [
    "an absolutely miniscule improvement\n",
    "\n",
    "let's try all words, not just max_features = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c66cfac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plurality: 0.6724 \n",
      "Baseline:  0.6724 \n",
      "All Words: 0.7005\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "all_words_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Plurality:', plurality_cv,\n",
    "      '\\nBaseline: ', baseline_cv,\n",
    "      '\\nAll Words:', all_words_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0050202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f5d9828",
   "metadata": {},
   "source": [
    "an actual improvement\n",
    "\n",
    "let's look at which 10 terms were least and most associated with positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a0da7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "# fit the vectorizer on X_train and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "# create array of the word list from this vectorizer with new index\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "# create array of the indices of the feature_names array, ordered by tfidf score\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "# create frequency distribution (dictionary) of 1-grams\n",
    "all_words_freq_dist = FreqDist(X_train['text_tokenized'].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06e7829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9622 worship\n",
      "0.9278 rocks\n",
      "0.9184 hmmmm\n",
      "0.9014 covet\n",
      "0.8927 orly\n",
      "0.8842 location\n",
      "0.8691 charging\n",
      "0.8686 whoooooo\n",
      "0.8622 applestore\n",
      "0.8585 deleting\n"
     ]
    }
   ],
   "source": [
    "# show the words with the top 10 tfidf values, and their tfidf values\n",
    "for n in range(-1,-11,-1):\n",
    "    print(round(X_train_vectorized.max(0).toarray()[0][sorted_tfidf_index[n]],4),\n",
    "          feature_names[sorted_tfidf_index[n]]\n",
    "         )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf02ea1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>i worship @mention {link} #sxsw</td>\n",
       "      <td>0</td>\n",
       "      <td>[worship, mention, link, sxsw]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text  label                  text_tokenized\n",
       "77  i worship @mention {link} #sxsw      0  [worship, mention, link, sxsw]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show all instances of this word\n",
    "X_train[X_train['text'].str.contains('worship')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4e7f358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['á¾_î¾ð' '_ã' '_ô' 'documents' '¼¼' 'çü' 'öý' 'èï' 'sxsw' 'karaoke']\n",
      "\n",
      "Largest tfidf: \n",
      "['worship' 'rocks' 'hmmmm' 'covet' 'orly' 'location' 'charging' 'whoooooo'\n",
      " 'applestore' 'deleting']\n"
     ]
    }
   ],
   "source": [
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133e8c6",
   "metadata": {},
   "source": [
    "we can and will explore stopwords, but it seems clear we can stem or lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28b2e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "def stem_and_tokenize(document):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5aa3f6",
   "metadata": {},
   "source": [
    "create stemmed vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8eb7b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of raw vocabulary:     8876\n",
      "Size of stemmed vocabulary: 7016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-918cbb3bc428>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:, 'text_stemmed'] = X_train.loc[:, 'text'].apply(stem_and_tokenize)\n"
     ]
    }
   ],
   "source": [
    "X_train.loc[:, 'text_stemmed'] = X_train.loc[:, 'text'].apply(stem_and_tokenize)\n",
    "vocab_stemmed = set(X_train['text_stemmed'].explode())\n",
    "print('Size of raw vocabulary:    ', len(vocab_raw))\n",
    "print('Size of stemmed vocabulary:', len(vocab_stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bdd48cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plurality:     0.6724 \n",
      "Baseline:      0.6724 \n",
      "All Words:     0.7005 \n",
      "Stemmed Words: 0.6995\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer = stem_and_tokenize\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "stemmed_words_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Plurality:    ', plurality_cv,\n",
    "      '\\nBaseline:     ', baseline_cv,\n",
    "      '\\nAll Words:    ', all_words_cv,\n",
    "      '\\nStemmed Words:', stemmed_words_cv\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65322da",
   "metadata": {},
   "source": [
    "Stemming is worse by about one tenth of a percent\n",
    "\n",
    "top 10 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7aa8d532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['¼¼' 'á¾_î¾ð' 'çü' 'èï' 'öý' '_ô' '_ã' 'primo' 'nowher' 'visto']\n",
      "\n",
      "Largest tfidf: \n",
      "['worship' 'hmmmm' 'rock' 'covet' 'locat' 'whoooooo' 'applestor' 'money'\n",
      " 'delet' 'atx']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba3c8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_and_tokenize(document):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d447e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of raw vocabulary:        8876 \n",
      "Size of stemmed vocabulary:    7016 \n",
      "Size of lemmatized vocabulary: 8208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-56-89c3d8203887>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:, 'text_lemmatized'] = X_train.loc[:, 'text'].apply(lemmatize_and_tokenize)\n"
     ]
    }
   ],
   "source": [
    "X_train.loc[:, 'text_lemmatized'] = X_train.loc[:, 'text'].apply(lemmatize_and_tokenize)\n",
    "vocab_lemmatized = set(X_train['text_lemmatized'].explode())\n",
    "print('Size of raw vocabulary:       ', len(vocab_raw),\n",
    "      '\\nSize of stemmed vocabulary:   ', len(vocab_stemmed),\n",
    "      '\\nSize of lemmatized vocabulary:', len(vocab_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c0a9783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plurality:        0.6724 \n",
      "All Words:        0.7005 \n",
      "Stemmed Words:    0.6995 \n",
      "Lemmatized Words: 0.6984\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer = lemmatize_and_tokenize\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "lemmatized_words_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Plurality:       ', plurality_cv,\n",
    "      '\\nAll Words:       ', all_words_cv,\n",
    "      '\\nStemmed Words:   ', stemmed_words_cv,\n",
    "      '\\nLemmatized Words:', lemmatized_words_cv\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d069132f",
   "metadata": {},
   "source": [
    "lemmatizing makes it worse by another tenth of a percent\n",
    "\n",
    "look at top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b284e507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['document' '_ã' 'öý' '_ô' 'èï' 'çü' 'á¾_î¾ð' '¼¼' 'primo' 'visto']\n",
      "\n",
      "Largest tfidf: \n",
      "['worship' 'hmmmm' 'rock' 'covet' 'orly' 'location' 'whoooooo' 'charging'\n",
      " 'applestore' 'deleting']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e783c17",
   "metadata": {},
   "source": [
    "explore bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6eea2",
   "metadata": {},
   "source": [
    "let's try n-grams, n from 2 to 7, using all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd84485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(word_list, n):\n",
    "    length = len(word_list)\n",
    "    if length < n:\n",
    "        return None\n",
    "    else:\n",
    "        ngram_list = []\n",
    "        for i in range(length - n + 1):\n",
    "            ngram = ''\n",
    "            for j in range(i, i+n):\n",
    "                if j > i:\n",
    "                    ngram += ' '\n",
    "                ngram += word_list[j]\n",
    "            ngram_list.append(ngram)\n",
    "        return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fbacb9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-76-cd7379d45510>:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:, str(n)+'_grams'] = X_train.loc[:, 'text_tokenized'].apply(lambda x: make_ngrams(x, n))\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range = (n,n)\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "score = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "\n",
    "\n",
    "# create array of the word list from this vectorizer with new index\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "# create array of the indices of the feature_names array, ordered by tfidf score\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "X_train.loc[:, str(n)+'_grams'] = X_train.loc[:, 'text_tokenized'].apply(lambda x: make_ngrams(x, n))\n",
    "\n",
    "# create frequency distribution (dictionary) of 1-grams\n",
    "bigrams_freq_dist = FreqDist(X_train['text_tokenized'].explode())\n",
    "\n",
    "bigrams = X_train.loc[:, str(n)+'_grams'].explode()\n",
    "bigrams_freq_dist = FreqDist(bigrams)\n",
    "    \n",
    "#     smallest.append(feature_names[sorted_tfidf_index[:10]])\n",
    "#     largest.append(feature_names[sorted_tfidf_index[:-11:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ae890e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 mention sxsw ipad rocks\n",
      "1.0 my sxsw iphone screen\n",
      "1.0 worship mention link sxsw\n",
      "1.0 essential sxsw tools link\n",
      "1.0 iphone sharing sxsw shareable\n",
      "1.0 google circles sxsw orly\n",
      "0.8038 at apple store at\n",
      "0.7342 ipad line sxsw link\n",
      "0.7229 in hand sxsw thisisdare\n",
      "0.7229 covet new ipad link\n"
     ]
    }
   ],
   "source": [
    "# show the words with the top 10 tfidf values, and their tfidf values\n",
    "for n in range(-1,-11,-1):\n",
    "    print(round(X_train_vectorized.max(0).toarray()[0][sorted_tfidf_index[n]],4),\n",
    "          feature_names[sorted_tfidf_index[n]]\n",
    "         )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a3a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "smallest, largest = [], []\n",
    "\n",
    "for n in range(1,8):\n",
    "    \n",
    "    if n > 1:\n",
    "        tfidf = TfidfVectorizer(\n",
    "            ngram_range = (n,n)\n",
    "        )\n",
    "    else:\n",
    "        tfidf = TfidfVectorizer()\n",
    "    \n",
    "    X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "    \n",
    "    score = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "    \n",
    "    print(str(n)+'-gram', 'score:', score)\n",
    "\n",
    "    feature_names = np.array(tfidf.get_feature_names())\n",
    "\n",
    "    sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "    \n",
    "    smallest.append(feature_names[sorted_tfidf_index[:10]])\n",
    "    largest.append(feature_names[sorted_tfidf_index[:-11:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cf8664",
   "metadata": {},
   "source": [
    "top ten only looks at top ten most frequent, so this is useless until you give it a stop words list, probably not useful at all\n",
    "\n",
    "now i have a way to find the top and bottom tfidf scores, i need to take this one step further to see how frequent those terms are, this will help me to decide to ignore n > 4 for example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff7f95",
   "metadata": {},
   "source": [
    "I will make a function to use with lambda to generate a column of ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cabfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(word_list, n):\n",
    "    length = len(word_list)\n",
    "    if length < n:\n",
    "        return None\n",
    "    else:\n",
    "        ngram_list = []\n",
    "        for i in range(length - n + 1):\n",
    "            ngram = ''\n",
    "            for j in range(i, i+n):\n",
    "                if j > i:\n",
    "                    ngram += ' '\n",
    "                ngram += word_list[j]\n",
    "            ngram_list.append(ngram)\n",
    "        return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc316c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it might stop throwing an error if you FIRST establish these columns, THEN assign values to them??\n",
    "for n in range(3,4):\n",
    "    title = str(n) + '_grams'\n",
    "    X_train.loc[:, title] = X_train.loc[:, 'text_tokenized'].apply(lambda x: make_ngrams(x, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = X_train.loc[:, '3_grams'].explode()\n",
    "all_words_freq_dist = FreqDist(all_words)\n",
    "\n",
    "all_words_set = set(all_words)\n",
    "\n",
    "all_words_ordered = list(zip(*all_words_freq_dist.most_common(10)))\n",
    "\n",
    "all_words_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ae3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in largest[2]:\n",
    "    print(all_words_freq_dist[item], item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "error pls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6404201",
   "metadata": {},
   "source": [
    "looks like n-grams help up to about n = 4, maybe more\n",
    "\n",
    "let's explore the top 10 n-grams for each n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063932ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(token_list):\n",
    "    stopwords_removed = [token for token in token_list if token not in stopwords_list]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3907e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"text_without_stopwords\"] = X_train[\"text_tokenized\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c394695",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10,\n",
    "    stop_words=stopwords_list\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\n",
    "\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_removed_cv = run_test(baseline_model, X_train_vectorized, y_train)\n",
    "stopwords_removed_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb30af",
   "metadata": {},
   "source": [
    "still very bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5117a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline:         \", baseline_cv.mean())\n",
    "print(\"Stopwords removed:\", stopwords_removed_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767015a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "def stem_and_tokenize(document):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_stopwords = [stemmer.stem(word) for word in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10,\n",
    "    stop_words=stemmed_stopwords,\n",
    "    tokenizer=stem_and_tokenize\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\n",
    "\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cd697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_cv = run_test(baseline_model, X_train_vectorized, y_train)\n",
    "stemmed_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988b8bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stopwords removed:\", stopwords_removed_cv.mean())\n",
    "print(\"Stemmed:          \", stemmed_cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674f35f",
   "metadata": {},
   "source": [
    "getting absolutely nowhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69541bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['num_words'] = X_train['text_tokenized'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccea21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"num_sentences\"] = X_train[\"text\"].apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6907a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"label\"] = [y_train[val] for val in X_train.index]\n",
    "\n",
    "def plot_words(column, title):\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    fig.set_tight_layout(True)\n",
    "    gs = fig.add_gridspec(1, 2)\n",
    "    ax1 = fig.add_subplot(gs[0, :1])\n",
    "    ax2 = fig.add_subplot(gs[0, 1:2])\n",
    "\n",
    "    axes = [ax1, ax2]\n",
    "\n",
    "    for index, category in enumerate(y_train.unique()):\n",
    "\n",
    "        all_words = X_train[X_train[\"label\"] == category][column].explode()\n",
    "        freq_dist = FreqDist(all_words)\n",
    "        top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "        tokens = top_10[0]\n",
    "        counts = top_10[1]\n",
    "\n",
    "        ax = axes[index]\n",
    "        ax.bar(tokens, counts)\n",
    "\n",
    "        ax.set_title(f\"{title} {category}\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax.tick_params(axis=\"x\", rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b627e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_words('text_without_stopwords', 'TITLE_HERE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e345b98",
   "metadata": {},
   "source": [
    "brainstorm feature engineering:\n",
    "\n",
    "- whether / how many times the product is mentioned\n",
    "- number of words\n",
    "- contains an emoji\n",
    "- look at bigrams (37.08)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba5ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_plus_features()\n",
    "\n",
    "pd.concat([\n",
    "    X_test_vectorized_df, X_test[[\"num_sentences\", \"contains_price\", \"conta ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977fe9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b57e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_bigram_finder = BigramCollocationFinder.from_words(macbeth_words_stopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccae93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc021b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd64c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398216c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f5b48dd",
   "metadata": {},
   "source": [
    "begin editing text\n",
    "start with tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b11d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train['text_tokenized'] = X_train['text'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ced584",
   "metadata": {},
   "source": [
    "explore complete vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a764ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = X_train['text_tokenized'].explode()\n",
    "all_words_freq_dist = FreqDist(all_words)\n",
    "\n",
    "all_words_set = set(all_words)\n",
    "\n",
    "all_words_ordered = list(zip(*all_words_freq_dist.most_common()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a5bbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfee5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41dfab90",
   "metadata": {},
   "source": [
    "separate pos from neg\n",
    "\n",
    "actually what I've done here is probably redundant to tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f1b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_words = X_train[X_train.label == 'positive']['text_tokenized'].explode()\n",
    "all_pos_words_freq_dist = FreqDist(all_pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54cd94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_words_set = set(all_pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3218985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_words_ordered = list(zip(*all_pos_words_freq_dist.most_common()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b3139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_non_words = X_train[X_train.label == 'not positive']['text_tokenized'].explode()\n",
    "all_non_words_freq_dist = FreqDist(all_non_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_non_words_set = set(all_non_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4305ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_non_words_ordered = list(zip(*all_non_words_freq_dist.most_common()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13287e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_differential = {}\n",
    "\n",
    "for word in all_words_set:\n",
    "    word_differential[word] = all_pos_words_freq_dist[word] - all_non_words_freq_dist[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b45fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(word_differential.items(), key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865bbd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_set - all_non_words_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395581b3",
   "metadata": {},
   "source": [
    "explore top ten frequency\n",
    "\n",
    "consider reprogramming this since my all_words stuff makes it partially redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a218ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_freq_dist = FreqDist(X_train[\"text_tokenized\"].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7200aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_top_10(freq_dist, title):\n",
    "    # Extract data for plotting\n",
    "    top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "    tokens = top_10[0]\n",
    "    counts = top_10[1]\n",
    "    \n",
    "    # Set up plot and plot data\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(tokens, counts)\n",
    "    \n",
    "    # Customize plot appearance\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    \n",
    "# visualize_top_10(train_freq_dist, \"Top 10 Most Common Words\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
