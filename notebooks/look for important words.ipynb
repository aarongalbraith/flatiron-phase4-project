{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d697513d",
   "metadata": {},
   "source": [
    "# This version will try to learn what the most important words are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054db9c2",
   "metadata": {},
   "source": [
    "import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a45af003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from operator import itemgetter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2f168",
   "metadata": {},
   "source": [
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3efcb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/df.csv')\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92822670",
   "metadata": {},
   "source": [
    "could do more here to explore the neither and both values\n",
    "\n",
    "move on to language processing\n",
    "\n",
    "train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe1c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['text'].to_frame(), df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e6a0d2",
   "metadata": {},
   "source": [
    "add label to X_train for research purposes .. obviously don't include this in the model\n",
    "\n",
    "reset index to anticipate future problems ... or not reset the index???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a99117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-0cb08c382a27>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:, 'label'] = [y_train.loc[val] for val in X_train.index]\n"
     ]
    }
   ],
   "source": [
    "X_train.loc[:, 'label'] = [y_train.loc[val] for val in X_train.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111688f4",
   "metadata": {},
   "source": [
    "perfunctory exploring should happen here\n",
    "\n",
    "top ten visualizations for pos. and non-pos.\n",
    "\n",
    "size of vocabulary\n",
    "\n",
    "more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec021d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "tokenizer = RegexpTokenizer(basic_token_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "911554b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of raw vocabulary: 8876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-5ba9b3884ac5>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:, 'text_tokenized'] = X_train['text'].apply(tokenizer.tokenize)\n"
     ]
    }
   ],
   "source": [
    "X_train.loc[:, 'text_tokenized'] = X_train['text'].apply(tokenizer.tokenize)\n",
    "vocab_raw = set(X_train['text_tokenized'].explode())\n",
    "print('Size of raw vocabulary:', len(vocab_raw))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fad27",
   "metadata": {},
   "source": [
    "gonna need naive bayes, might not do any other models (markov, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ebbc87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce4838",
   "metadata": {},
   "source": [
    "look at plurality winner to see score to beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3f8d3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    0.672366\n",
       "1    0.327634\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plurality_cv = round(y_train.value_counts(normalize=True)[0],4)\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83dc69",
   "metadata": {},
   "source": [
    "first model, just ten features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "525fa18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plurality: 0.6724 \n",
      "Baseline:  0.6724\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features = 10\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "baseline_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Plurality:', plurality_cv,\n",
    "      '\\nBaseline: ',baseline_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84865a1c",
   "metadata": {},
   "source": [
    "an absolutely miniscule improvement\n",
    "\n",
    "let's try all words, not just max_features = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9fd2834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plurality: 0.6724 \n",
      "Baseline:  0.6724 \n",
      "All Words: 0.7005\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "all_words_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Plurality:', plurality_cv,\n",
    "      '\\nBaseline: ', baseline_cv,\n",
    "      '\\nAll Words:', all_words_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d6c1a",
   "metadata": {},
   "source": [
    "an actual improvement\n",
    "\n",
    "let's look at which 10 terms were least and most associated with positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604dc2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the word list from this vectorizer with new index\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "# create array of the indices of the feature_names array, ordered by tfidf score\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "# create frequency distribution (dictionary) of 1-grams\n",
    "all_words_freq_dist = FreqDist(X_train['text_tokenized'].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "511ee62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf | frequency | word\n",
      "\n",
      "0.9622 1 worship\n",
      "0.9278 10 rocks\n",
      "0.9184 4 hmmmm\n",
      "0.9014 1 covet\n",
      "0.8927 2 orly\n",
      "0.8842 106 location\n",
      "0.8691 15 charging\n",
      "0.8686 1 whoooooo\n",
      "0.8622 5 applestore\n",
      "0.8585 2 deleting\n",
      "0.8434 24 atx\n",
      "0.8336 41 oh\n",
      "0.8262 13 circle\n",
      "0.8253 1 craziness\n",
      "0.8168 1 shice\n",
      "0.8146 37 money\n",
      "0.8111 2 denotes\n",
      "0.8104 36 watch\n",
      "0.8091 2382 rt\n",
      "0.806 37 screen\n"
     ]
    }
   ],
   "source": [
    "# show the words with the top 10 tfidf values, and their tfidf values\n",
    "print('tfidf | frequency | word\\n')\n",
    "for n in range(-1,-21,-1):\n",
    "    index = sorted_tfidf_index[n]\n",
    "    print(\n",
    "        round(X_train_vectorized.max(0).toarray()[0][index],4),\n",
    "        all_words_freq_dist[feature_names[index]],\n",
    "        feature_names[index]\n",
    "         )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c935e1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf | frequency | word\n",
      "\n",
      "0.2333 1 á¾_î¾ð\n",
      "0.2333 1 _ã\n",
      "0.2333 1 _ô\n",
      "0.2333 1 documents\n",
      "0.2333 1 ¼¼\n",
      "0.2333 1 çü\n",
      "0.2333 1 öý\n",
      "0.2333 1 èï\n",
      "0.2341 7685 sxsw\n",
      "0.2358 1 karaoke\n"
     ]
    }
   ],
   "source": [
    "# show the words with the bottom 10 tfidf values, and their tfidf values\n",
    "print('tfidf | frequency | word\\n')\n",
    "for n in range(10):\n",
    "    index = sorted_tfidf_index[n]\n",
    "    print(\n",
    "        round(X_train_vectorized.max(0).toarray()[0][index],4),\n",
    "        all_words_freq_dist[feature_names[index]],\n",
    "        feature_names[index]\n",
    "         )      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d43be",
   "metadata": {},
   "source": [
    "experiment with min_df and max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "553d33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_df = 5\n",
    ")\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "# create the word list from this vectorizer with new index\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "# create array of the indices of the feature_names array, ordered by tfidf score\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "# create frequency distribution (dictionary) of 1-grams\n",
    "all_words_freq_dist = FreqDist(X_train['text_tokenized'].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a31a6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf | frequency | doc frequency | word\n",
      "\n",
      "1.0 5 5 crashes\n",
      "1.0 3 3 mccannsxsw\n",
      "1.0 1 1 recipient\n",
      "1.0 2 2 recipes\n",
      "1.0 3 3 huh\n",
      "1.0 1 1 sxswbarcrawl\n",
      "1.0 3 4 evil\n",
      "1.0 5 5 sxswbuffalo\n",
      "1.0 1 3 damm\n",
      "1.0 3 3 mcommerce\n",
      "1.0 2 2 sxswedu\n",
      "1.0 1 2 blah\n",
      "1.0 4 7 hrs\n",
      "1.0 4 4 mcree\n",
      "1.0 2 3 sxswgo\n",
      "1.0 1 1 sxswgood\n",
      "1.0 2 2 blast\n",
      "1.0 1 1 sxswk\n",
      "1.0 5 5 elusive\n",
      "1.0 5 5 mechanics\n"
     ]
    }
   ],
   "source": [
    "# show the words with the top 10 tfidf values, and their tfidf values\n",
    "print('tfidf | frequency | doc frequency | word\\n')\n",
    "for n in range(-1,-21,-1):\n",
    "    index = sorted_tfidf_index[n]\n",
    "    print(\n",
    "        round(X_train_vectorized.max(0).toarray()[0][index],4),\n",
    "        all_words_freq_dist[feature_names[index]],\n",
    "        len(X_train[X_train['text'].str.contains(feature_names[index])]),\n",
    "        feature_names[index]\n",
    "         )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6668c06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf | frequency | doc frequency | word\n",
      "\n",
      "0.2646 1 1 çü\n",
      "0.2646 1 1 èï\n",
      "0.2646 1 1 _ô\n",
      "0.2646 1 1 _ã\n",
      "0.2646 1 1 á¾_î¾ð\n",
      "0.2646 1 1 öý\n",
      "0.2646 1 1 documents\n",
      "0.2646 1 1 ¼¼\n",
      "0.2748 1 1 destroyed\n",
      "0.2748 1 1 trajan\n"
     ]
    }
   ],
   "source": [
    "# show the words with the bottom 10 tfidf values, and their tfidf values\n",
    "print('tfidf | frequency | doc frequency | word\\n')\n",
    "for n in range(10):\n",
    "    index = sorted_tfidf_index[n]\n",
    "    print(\n",
    "        round(X_train_vectorized.max(0).toarray()[0][index],4),\n",
    "        all_words_freq_dist[feature_names[index]],\n",
    "        len(X_train[X_train['text'].str.contains(feature_names[index])]),\n",
    "        feature_names[index]\n",
    "         )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed425552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>i worship @mention {link} #sxsw</td>\n",
       "      <td>0</td>\n",
       "      <td>[worship, mention, link, sxsw]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text  label                  text_tokenized\n",
       "76  i worship @mention {link} #sxsw      0  [worship, mention, link, sxsw]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show all instances of this word\n",
    "X_train[X_train['text'].str.contains('worship')]\n",
    "# the number of instances will often differ from the frequency\n",
    "# because this will catch instances when this is juxtaposed with more characters\n",
    "# e.g. 'covet' & 'coveting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97dbabc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['çü' 'èï' '_ô' '_ã' 'á¾_î¾ð' 'öý' 'documents' '¼¼' 'destroyed' 'trajan']\n",
      "\n",
      "Largest tfidf: \n",
      "['crashes' 'mccannsxsw' 'recipient' 'recipes' 'huh' 'sxswbarcrawl' 'evil'\n",
      " 'sxswbuffalo' 'damm' 'mcommerce']\n"
     ]
    }
   ],
   "source": [
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6efb61d",
   "metadata": {},
   "source": [
    "explore bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f01fc8",
   "metadata": {},
   "source": [
    "let's try n-grams, n from 2 to 7, using all words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23eb469",
   "metadata": {},
   "source": [
    "top ten only looks at top ten most frequent, so this is useless until you give it a stop words list, probably not useful at all\n",
    "\n",
    "now i have a way to find the top and bottom tfidf scores, i need to take this one step further to see how frequent those terms are, this will help me to decide to ignore n > 4 for example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12966df",
   "metadata": {},
   "source": [
    "I will make a function to use with lambda to generate a column of ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38d4fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_words = X_train[X_train.label == 1]['text_tokenized'].explode()\n",
    "all_pos_words_freq_dist = FreqDist(all_pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4a0e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_words_set = set(all_pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe0c0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_words_ordered = list(zip(*all_pos_words_freq_dist.most_common()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75b27486",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_non_words = X_train[X_train.label == 0]['text_tokenized'].explode()\n",
    "all_non_words_freq_dist = FreqDist(all_non_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a41905b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_non_words_set = set(all_non_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ba640f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_non_words_ordered = list(zip(*all_non_words_freq_dist.most_common()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c3fc1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pos = np.mean(list(all_pos_words_freq_dist.values()))\n",
    "s_pos = np.std(list(all_pos_words_freq_dist.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e9d7dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_words_freq_dist_normalized = {}\n",
    "\n",
    "for key in all_pos_words_freq_dist:\n",
    "    all_pos_words_freq_dist_normalized[key] = (all_pos_words_freq_dist[key] - m_pos) / s_pos\n",
    "    \n",
    "\n",
    "# all_pos_words_freq_dist_normalized = (all_pos_words_freq_dist - m) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c3a4ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_non = np.mean(list(all_non_words_freq_dist.values()))\n",
    "s_non = np.std(list(all_non_words_freq_dist.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "723627f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_non_words_freq_dist_normalized = {}\n",
    "\n",
    "for key in all_non_words_freq_dist:\n",
    "    all_non_words_freq_dist_normalized[key] = (all_non_words_freq_dist[key] - m_non) / s_non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3b1458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_differential = {}\n",
    "word_ratio = {}\n",
    "\n",
    "for word in vocab_raw:\n",
    "    a, b = 0, 0\n",
    "    if word in all_pos_words_freq_dist_normalized:\n",
    "        a = all_pos_words_freq_dist_normalized[word]\n",
    "    if word in all_non_words_freq_dist_normalized:\n",
    "        b = all_non_words_freq_dist_normalized[word]\n",
    "    word_differential[word] = a - b\n",
    "    if a * b != 0:\n",
    "        word_ratio[word] = a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5598e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(word_differential.keys())\n",
    "values = list(word_differential.values())\n",
    "sorted_value_index = np.argsort(values)\n",
    "sorted_dict = {keys[i]: values[i] for i in sorted_value_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35cb3025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5918 1 xooms\n",
      "0.4749 1 adpeopleproblems\n",
      "1.0 1 lepanele\n",
      "0.4218 1 moore\n",
      "0.4511 1 lecture\n",
      "0.4472 1 alacagimiz\n",
      "1.0 1 omarg\n",
      "0.3873 1 imthatgood\n",
      "0.4736 1 huuray\n",
      "0.5774 1 moleskin\n",
      "0.5965 1 digits\n",
      "0.3766 1 compute\n",
      "0.4304 1 mis\n",
      "0.3622 1 dailydeals\n",
      "0.5918 1 popupstores\n",
      "0.4643 1 spaces\n",
      "0.7342 1 manufacturing\n",
      "0.5918 1 ser\n",
      "0.3402 1 equates\n",
      "0.4472 1 whats\n",
      "1.0 1 wnt\n",
      "0.5774 1 instrumental\n",
      "0.5774 1 ami\n",
      "0.7071 1 venn\n",
      "0.5111 1 coveting\n",
      "0.5223 1 armed\n",
      "0.4538 1 objects\n",
      "0.5774 1 contextclues\n",
      "0.4593 1 49n4m\n",
      "0.3325 1 shindig\n",
      "0.4673 1 zzzs\n",
      "1.0 1 coudbeeasier\n",
      "0.5774 1 30k\n",
      "1.0 1 producs\n",
      "0.5147 1 marks\n",
      "0.5192 1 sizzle\n",
      "0.5 1 gingerman\n",
      "0.5286 1 ûólots\n",
      "0.5093 1 sitter\n",
      "0.5055 1 hits\n",
      "0.5774 1 appropriately\n",
      "0.3894 1 crisiscommons\n",
      "0.4511 1 rose\n",
      "0.4301 1 burlesque\n",
      "1.0 1 appleapple\n",
      "1.0 1 arboretum\n",
      "0.4112 1 snubor\n",
      "0.7071 1 iphoneapp\n",
      "0.3552 1 divorces\n",
      "0.7229 1 n14\n",
      "1.0 1 event_iap5507\n",
      "0.3938 1 sketchily\n",
      "0.3949 1 timberlake\n",
      "0.4725 1 activate\n",
      "0.5223 1 bluefly\n",
      "0.7071 1 disturbing\n",
      "0.5 1 motley\n",
      "0.5206 1 pieces\n",
      "0.3627 1 manual\n",
      "0.5391 1 ep3\n",
      "0.7071 1 iv\n",
      "0.6003 1 precautions\n",
      "0.3897 1 jayden\n",
      "0.5147 1 ringer\n",
      "0.7505 1 austinl\n",
      "0.3844 1 watson\n",
      "0.7071 1 implement\n",
      "0.3147 1 mat\n",
      "0.4472 1 oldu\n",
      "0.3369 1 nite\n",
      "0.5312 1 scope\n",
      "0.4231 1 apartmovie\n",
      "0.4367 1 ehphone\n",
      "0.4472 1 sehugg\n",
      "0.601 1 aligned\n",
      "0.4211 1 bio\n",
      "0.5774 1 smack\n",
      "0.7342 1 mentionm\n",
      "0.7229 1 waaaambulance\n",
      "0.5223 1 icm\n",
      "1.0 1 mashabl\n",
      "0.5268 1 abilities\n",
      "0.7229 1 veriphone\n",
      "0.5123 1 chart\n",
      "0.3927 1 yyz\n",
      "1.0 1 bundle\n",
      "0.5947 1 memes\n",
      "0.7342 1 surprisingly\n",
      "1.0 1 _really_\n",
      "0.7432 1 denis\n",
      "0.7342 1 webby\n",
      "0.3435 1 visited\n",
      "0.3827 1 crowdsource\n",
      "0.7071 1 cope\n",
      "0.5147 1 nerdier\n",
      "0.5055 1 thestreet\n",
      "0.5147 1 cheatday\n",
      "1.0 1 450\n",
      "0.7342 1 billings\n",
      "0.7505 1 idle\n",
      "0.5286 1 quoted\n",
      "0.5 1 nnnneeerrrrddddsss\n",
      "0.5206 1 letsdothis\n",
      "1.0 1 assumes\n",
      "0.3588 1 nigerian\n",
      "1.0 1 specifically\n",
      "0.5182 1 10k\n",
      "0.4142 1 socialnetworks\n",
      "0.7342 1 eyes\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n",
      "0.5774 1 lowtech\n"
     ]
    }
   ],
   "source": [
    "for word in list(sorted_dict)[-201:]:\n",
    "    for i in range(len(feature_names)):\n",
    "        if feature_names[i] == word:\n",
    "            index = i\n",
    "    print(\n",
    "        round(X_train_vectorized.max(0).toarray()[0][index],4),\n",
    "        all_words_freq_dist[feature_names[index]],\n",
    "        feature_names[index]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b586dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mention',\n",
       " 'sxsw',\n",
       " 'link',\n",
       " 'google',\n",
       " 'rt',\n",
       " 'quot',\n",
       " 'to',\n",
       " 'at',\n",
       " 'circles',\n",
       " 'social']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted_dict)[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
