{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05fb4d11",
   "metadata": {},
   "source": [
    "# This version will try to learn what the most important words are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340752b",
   "metadata": {},
   "source": [
    "import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97880170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from operator import itemgetter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01e3dd7",
   "metadata": {},
   "source": [
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8cc10c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/df.csv')\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93bee56",
   "metadata": {},
   "source": [
    "could do more here to explore the neither and both values\n",
    "\n",
    "move on to language processing\n",
    "\n",
    "train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae642623",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['text'].to_frame(), df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabfef0f",
   "metadata": {},
   "source": [
    "add label to X_train for research purposes .. obviously don't include this in the model\n",
    "\n",
    "reset index to anticipate future problems ... or not reset the index???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "357a3329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-0cb08c382a27>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:, 'label'] = [y_train.loc[val] for val in X_train.index]\n"
     ]
    }
   ],
   "source": [
    "X_train.loc[:, 'label'] = [y_train.loc[val] for val in X_train.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40822ef1",
   "metadata": {},
   "source": [
    "perfunctory exploring should happen here\n",
    "\n",
    "top ten visualizations for pos. and non-pos.\n",
    "\n",
    "size of vocabulary\n",
    "\n",
    "more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70d064cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "tokenizer = RegexpTokenizer(basic_token_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35f9fc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of raw vocabulary: 8876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-5ba9b3884ac5>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:, 'text_tokenized'] = X_train['text'].apply(tokenizer.tokenize)\n"
     ]
    }
   ],
   "source": [
    "X_train.loc[:, 'text_tokenized'] = X_train['text'].apply(tokenizer.tokenize)\n",
    "vocab_raw = set(X_train['text_tokenized'].explode())\n",
    "print('Size of raw vocabulary:', len(vocab_raw))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97280035",
   "metadata": {},
   "source": [
    "gonna need naive bayes, might not do any other models (markov, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1af99133",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e164d019",
   "metadata": {},
   "source": [
    "look at plurality winner to see score to beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aac2ea64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    0.672366\n",
       "1    0.327634\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plurality_cv = round(y_train.value_counts(normalize=True)[0],4)\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab8bcf",
   "metadata": {},
   "source": [
    "first model, just ten features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e23cea5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plurality: 0.6724 \n",
      "Baseline:  0.6724\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features = 10\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "baseline_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Plurality:', plurality_cv,\n",
    "      '\\nBaseline: ',baseline_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0b635",
   "metadata": {},
   "source": [
    "an absolutely miniscule improvement\n",
    "\n",
    "let's try all words, not just max_features = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "247fc7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plurality: 0.6724 \n",
      "Baseline:  0.6724 \n",
      "All Words: 0.7005\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "all_words_cv = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "print('Plurality:', plurality_cv,\n",
    "      '\\nBaseline: ', baseline_cv,\n",
    "      '\\nAll Words:', all_words_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e10b7d",
   "metadata": {},
   "source": [
    "an actual improvement\n",
    "\n",
    "let's look at which 10 terms were least and most associated with positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e28997fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "# fit the vectorizer on X_train and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "# create array of the word list from this vectorizer with new index\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "# create array of the indices of the feature_names array, ordered by tfidf score\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "# create frequency distribution (dictionary) of 1-grams\n",
    "all_words_freq_dist = FreqDist(X_train['text_tokenized'].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a52d079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9622 worship\n",
      "0.9278 rocks\n",
      "0.9184 hmmmm\n",
      "0.9014 covet\n",
      "0.8927 orly\n",
      "0.8842 location\n",
      "0.8691 charging\n",
      "0.8686 whoooooo\n",
      "0.8622 applestore\n",
      "0.8585 deleting\n"
     ]
    }
   ],
   "source": [
    "# show the words with the top 10 tfidf values, and their tfidf values\n",
    "for n in range(-1,-11,-1):\n",
    "    print(round(X_train_vectorized.max(0).toarray()[0][sorted_tfidf_index[n]],4),\n",
    "          feature_names[sorted_tfidf_index[n]]\n",
    "         )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "759ece4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>i worship @mention {link} #sxsw</td>\n",
       "      <td>0</td>\n",
       "      <td>[worship, mention, link, sxsw]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text  label                  text_tokenized\n",
       "77  i worship @mention {link} #sxsw      0  [worship, mention, link, sxsw]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show all instances of this word\n",
    "X_train[X_train['text'].str.contains('worship')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f7625df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['á¾_î¾ð' '_ã' '_ô' 'documents' '¼¼' 'çü' 'öý' 'èï' 'sxsw' 'karaoke']\n",
      "\n",
      "Largest tfidf: \n",
      "['worship' 'rocks' 'hmmmm' 'covet' 'orly' 'location' 'charging' 'whoooooo'\n",
      " 'applestore' 'deleting']\n"
     ]
    }
   ],
   "source": [
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6176c9",
   "metadata": {},
   "source": [
    "explore bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7d44ab",
   "metadata": {},
   "source": [
    "let's try n-grams, n from 2 to 7, using all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d669543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(word_list, n):\n",
    "    length = len(word_list)\n",
    "    if length < n:\n",
    "        return None\n",
    "    else:\n",
    "        ngram_list = []\n",
    "        for i in range(length - n + 1):\n",
    "            ngram = ''\n",
    "            for j in range(i, i+n):\n",
    "                if j > i:\n",
    "                    ngram += ' '\n",
    "                ngram += word_list[j]\n",
    "            ngram_list.append(ngram)\n",
    "        return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "26521d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-76-cd7379d45510>:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:, str(n)+'_grams'] = X_train.loc[:, 'text_tokenized'].apply(lambda x: make_ngrams(x, n))\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range = (n,n)\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "\n",
    "score = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "\n",
    "\n",
    "# create array of the word list from this vectorizer with new index\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "# create array of the indices of the feature_names array, ordered by tfidf score\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "X_train.loc[:, str(n)+'_grams'] = X_train.loc[:, 'text_tokenized'].apply(lambda x: make_ngrams(x, n))\n",
    "\n",
    "# create frequency distribution (dictionary) of 1-grams\n",
    "bigrams_freq_dist = FreqDist(X_train['text_tokenized'].explode())\n",
    "\n",
    "bigrams = X_train.loc[:, str(n)+'_grams'].explode()\n",
    "bigrams_freq_dist = FreqDist(bigrams)\n",
    "    \n",
    "#     smallest.append(feature_names[sorted_tfidf_index[:10]])\n",
    "#     largest.append(feature_names[sorted_tfidf_index[:-11:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e2658862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 mention sxsw ipad rocks\n",
      "1.0 my sxsw iphone screen\n",
      "1.0 worship mention link sxsw\n",
      "1.0 essential sxsw tools link\n",
      "1.0 iphone sharing sxsw shareable\n",
      "1.0 google circles sxsw orly\n",
      "0.8038 at apple store at\n",
      "0.7342 ipad line sxsw link\n",
      "0.7229 in hand sxsw thisisdare\n",
      "0.7229 covet new ipad link\n"
     ]
    }
   ],
   "source": [
    "# show the words with the top 10 tfidf values, and their tfidf values\n",
    "for n in range(-1,-11,-1):\n",
    "    print(round(X_train_vectorized.max(0).toarray()[0][sorted_tfidf_index[n]],4),\n",
    "          feature_names[sorted_tfidf_index[n]]\n",
    "         )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e8a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "smallest, largest = [], []\n",
    "\n",
    "for n in range(1,8):\n",
    "    \n",
    "    if n > 1:\n",
    "        tfidf = TfidfVectorizer(\n",
    "            ngram_range = (n,n)\n",
    "        )\n",
    "    else:\n",
    "        tfidf = TfidfVectorizer()\n",
    "    \n",
    "    X_train_vectorized = tfidf.fit_transform(X_train['text'])\n",
    "    \n",
    "    score = round(cross_val_score(baseline_model, X_train_vectorized, y_train).mean(),4)\n",
    "    \n",
    "    print(str(n)+'-gram', 'score:', score)\n",
    "\n",
    "    feature_names = np.array(tfidf.get_feature_names())\n",
    "\n",
    "    sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "    \n",
    "    smallest.append(feature_names[sorted_tfidf_index[:10]])\n",
    "    largest.append(feature_names[sorted_tfidf_index[:-11:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060785b1",
   "metadata": {},
   "source": [
    "top ten only looks at top ten most frequent, so this is useless until you give it a stop words list, probably not useful at all\n",
    "\n",
    "now i have a way to find the top and bottom tfidf scores, i need to take this one step further to see how frequent those terms are, this will help me to decide to ignore n > 4 for example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5567a5b6",
   "metadata": {},
   "source": [
    "I will make a function to use with lambda to generate a column of ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f844ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(word_list, n):\n",
    "    length = len(word_list)\n",
    "    if length < n:\n",
    "        return None\n",
    "    else:\n",
    "        ngram_list = []\n",
    "        for i in range(length - n + 1):\n",
    "            ngram = ''\n",
    "            for j in range(i, i+n):\n",
    "                if j > i:\n",
    "                    ngram += ' '\n",
    "                ngram += word_list[j]\n",
    "            ngram_list.append(ngram)\n",
    "        return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137bde0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it might stop throwing an error if you FIRST establish these columns, THEN assign values to them??\n",
    "for n in range(3,4):\n",
    "    title = str(n) + '_grams'\n",
    "    X_train.loc[:, title] = X_train.loc[:, 'text_tokenized'].apply(lambda x: make_ngrams(x, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = X_train.loc[:, '3_grams'].explode()\n",
    "all_words_freq_dist = FreqDist(all_words)\n",
    "\n",
    "all_words_set = set(all_words)\n",
    "\n",
    "all_words_ordered = list(zip(*all_words_freq_dist.most_common(10)))\n",
    "\n",
    "all_words_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f472ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in largest[2]:\n",
    "    print(all_words_freq_dist[item], item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
